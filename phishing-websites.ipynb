{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Websites\n",
    "\n",
    "\n",
    "## About the Dataset\n",
    "This dataset consists of 11055 training data set and 2456 testing data set. It has 30 attributes, namely, \n",
    "- having_IP_Address  { -1,1 }\n",
    "- URL_Length   { 1,0,-1 } \n",
    "- Shortining_Service { 1,-1 } \n",
    "- having_At_Symbol   { 1,-1 } \n",
    "- double_slash_redirecting { -1,1 } \n",
    "- Prefix_Suffix  { -1,1 } \n",
    "- having_Sub_Domain  { -1,0,1 } \n",
    "- SSLfinal_State  { -1,1,0 } \n",
    "- Domain_registeration_length { -1,1 } \n",
    "- Favicon { 1,-1 } \n",
    "- port { 1,-1 } \n",
    "- HTTPS_token { -1,1 } \n",
    "- Request_URL  { 1,-1 } \n",
    "- URL_of_Anchor { -1,0,1 } \n",
    "- Links_in_tags { 1,-1,0 } \n",
    "- SFH  { -1,1,0 } \n",
    "- Submitting_to_email { -1,1 } \n",
    "- Abnormal_URL { -1,1 }\n",
    "- Redirect  { 0,1 } \n",
    "- on_mouseover  { 1,-1 }\n",
    "- RightClick  { 1,-1 } \n",
    "- popUpWidnow  { 1,-1 } \n",
    "- Iframe { 1,-1 } \n",
    "- age_of_domain  { -1,1 } \n",
    "- DNSRecord   { -1,1 } \n",
    "- web_traffic  { -1,0,1 } \n",
    "- Page_Rank { -1,1 } \n",
    "- Google_Index { 1,-1 } \n",
    "- Links_pointing_to_page { 1,0,-1 } \n",
    "- Statistical_report { -1,1 } \n",
    "- Result  { -1,1 } \n",
    "\n",
    "which they take on the values 1, 0, -1 that mean whether the website is either legitimate, suspicious or phishing, respectively.\n",
    "\n",
    "###### The following segment of code just creates and duplicates the dataset in the .arff files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Getting all the arff files from the current directory\n",
    "files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
    "\n",
    "# Function for converting arff list to csv list\n",
    "def toCsv(content):\n",
    "    data = False\n",
    "    header = \"\"\n",
    "    newContent = []\n",
    "    for line in content:\n",
    "        if not data:\n",
    "            if \"@attribute\" in line:\n",
    "                attri = line.split()\n",
    "                columnName = attri[attri.index(\"@attribute\")+1]\n",
    "                header = header + columnName + \",\"\n",
    "            elif \"@data\" in line:\n",
    "                data = True\n",
    "                header = header[:-1]\n",
    "                header += '\\n'\n",
    "                newContent.append(header)\n",
    "        else:\n",
    "            newContent.append(line)\n",
    "    return newContent\n",
    "\n",
    "# Main loop for reading and writing files\n",
    "for file in files:\n",
    "    with open(file , \"r\") as inFile:\n",
    "        content = inFile.readlines()\n",
    "        name,ext = os.path.splitext(inFile.name)\n",
    "        new = toCsv(content)\n",
    "        with open(name+\".csv\", \"w\") as outFile:\n",
    "            outFile.writelines(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset : \n",
      "number of samples_______ = 11055\n",
      "number of negative ones_ = 4898\n",
      "number of ones__________ = 6157\n",
      "testing dataset : \n",
      "number of samples_______ = 2456\n",
      "number of negative ones_ = 1362\n",
      "number of ones__________ = 1094\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"white\")\n",
    "sns.set(style = \"whitegrid\", color_codes = True)\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('Training Dataset.csv', sep = ',')\n",
    "data_test = pd.read_csv('old.csv', sep = ',')\n",
    "\n",
    "feature_index_names = {0:'having_IP_Address', 1:'URL_Length', 2:'Shortining_Service', 3:'having_At_Symbol', \n",
    "                        4:'double_slash_redirecting', 5:'Prefix_Suffix', 6:'having_Sub_Domain', 7:'SSLfinal_State',\n",
    "                        8:'Domain_registeration_length', 9:'Favicon', 10:'port', 11:'HTTPS_token',12:'Request_URL', \n",
    "                        13:'URL_of_Anchor', 14:'Links_in_tags', 15:'SFH', 16:'Submitting_to_email', 17:'Abnormal_URL', \n",
    "                        18:'Redirect', 19:'on_mouseover', 20:'RightClick', 21:'popUpWidnow', 22:'Iframe',\n",
    "                        23:' age_of_domain', 24:'DNSRecord', 25:'web_traffic', 26:'Page_Rank', 27:' Google_Index', \n",
    "                        28:'Links_pointing_to_page', 29:'Statistical_report'} \n",
    "#training dataset\n",
    "training_data = np.array(data_train)\n",
    "x_training = training_data[:, :-1]\n",
    "y_training = training_data[:, -1]\n",
    "\n",
    "print('training dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_training.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_training == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_training == 1)))\n",
    "\n",
    "#testing dataset\n",
    "testing_data = np.array(data_test)\n",
    "x_testing = testing_data[:, :-1]\n",
    "y_testing = testing_data[:, -1]\n",
    "\n",
    "print('testing dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_testing.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_testing == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_testing == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "We can also use logistic regression to perform occupancy detection. In order to achieve this, first we need to define our hypothesis (or model):\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "h_{\\boldsymbol{\\Theta}}(\\mathbf{x}) & = & \\frac{1}{1 + e^{-(\\theta_{0} x_{0} + \\theta_{1} x_{1} + \\cdots + \\theta_{d} x_{d})}} \\\\\n",
    "& = & \\frac{1}{1 + e^{-\\mathbf{x} \\boldsymbol{\\Theta}^{T}}},\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\mathbf{x} = [x_{0}, x_{1}, \\ldots, x_{d}]$, $\\boldsymbol{\\Theta}=[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{d}]$ and $x_{0} = 1$. \n",
    "\n",
    "The following function implements this model, which assumes that all vectors are row-major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, theta):\n",
    "    s = np.dot(x, theta.T)\n",
    "    u = 1.0 / (1.0 + np.exp(-s))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define our loss function to learn the parameters of the model based on training dataset. We can use mean square error as our loss function, i.e.,\n",
    "\n",
    "$$\n",
    "J\\left(\\boldsymbol{\\Theta}\\right) = \\frac{1}{2 N} \\sum\\limits_{n=1}^{N} \\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) - y^{\\left(n\\right)} \\right)^2 = \\frac{1}{N} \\sum\\limits_{n=1}^{N}Cost\\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) , y^{\\left(n\\right)}\\right).\n",
    "$$\n",
    "\n",
    "The following function implements the loss function $J\\left(\\boldsymbol{\\Theta}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, theta):\n",
    "    N = y.shape[0]\n",
    "    mse = 1.0 / (2*N) * np.sum((h(x, theta) - y)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need implement gradient descent solver to learn the parameters $\\boldsymbol{\\Theta}$ of our model. The following function implements batch gradient descent to learn the parameters $\\boldsymbol{\\Theta}$ on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(x, y, theta, alpha=0.00155, epsilon=0.001, max_iter=1000):\n",
    "    y = y[:, np.newaxis]\n",
    "    N = y.shape[0]\n",
    "    t = 0\n",
    "    while True:\n",
    "        # print the value of loss function for each iteration\n",
    "        print('iteration #{:>8d}, loss = {:>8f}'.format(t, J(x,y,theta)))\n",
    "        # keep a copy of the parameter vector before the update for checking the convergence criterion\n",
    "        theta_previous = theta.copy()\n",
    "        # update the parameter vector\n",
    "        e = (h(x, theta) - y)\n",
    "        theta  = theta - alpha * 1.0 / N * np.sum( e * x, axis=0)\n",
    "        t = t + 1\n",
    "        # check the convergence criterion\n",
    "        if (np.max(np.abs(theta-theta_previous)) < epsilon) or (t>max_iter):\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(x_training, axis=0)\n",
    "s = np.std(x_training, axis=0)\n",
    "x_training = (x_training - m) / s\n",
    "x_testing = (x_testing - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment training and testing vector to take advantage of fast matrix operations in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_aug = np.hstack((np.ones((x_training.shape[0],1)), x_training))\n",
    "x_testing_aug = np.hstack((np.ones((x_testing.shape[0],1)), x_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random parameter vector and learn the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #       0, loss = 0.727020\n",
      "iteration #       1, loss = 0.726745\n",
      "iteration #       2, loss = 0.726469\n",
      "iteration #       3, loss = 0.726194\n",
      "iteration #       4, loss = 0.725919\n",
      "iteration #       5, loss = 0.725643\n",
      "iteration #       6, loss = 0.725367\n",
      "iteration #       7, loss = 0.725092\n",
      "iteration #       8, loss = 0.724816\n",
      "iteration #       9, loss = 0.724540\n",
      "iteration #      10, loss = 0.724265\n",
      "iteration #      11, loss = 0.723989\n",
      "iteration #      12, loss = 0.723713\n",
      "iteration #      13, loss = 0.723437\n",
      "iteration #      14, loss = 0.723161\n",
      "iteration #      15, loss = 0.722885\n",
      "iteration #      16, loss = 0.722609\n",
      "iteration #      17, loss = 0.722333\n",
      "iteration #      18, loss = 0.722057\n",
      "iteration #      19, loss = 0.721781\n",
      "iteration #      20, loss = 0.721504\n",
      "iteration #      21, loss = 0.721228\n",
      "iteration #      22, loss = 0.720952\n",
      "iteration #      23, loss = 0.720675\n",
      "iteration #      24, loss = 0.720399\n",
      "iteration #      25, loss = 0.720122\n",
      "iteration #      26, loss = 0.719846\n",
      "iteration #      27, loss = 0.719569\n",
      "iteration #      28, loss = 0.719292\n",
      "iteration #      29, loss = 0.719016\n",
      "iteration #      30, loss = 0.718739\n",
      "iteration #      31, loss = 0.718462\n",
      "iteration #      32, loss = 0.718185\n",
      "iteration #      33, loss = 0.717909\n",
      "iteration #      34, loss = 0.717632\n",
      "iteration #      35, loss = 0.717355\n",
      "iteration #      36, loss = 0.717078\n",
      "iteration #      37, loss = 0.716801\n",
      "iteration #      38, loss = 0.716524\n",
      "iteration #      39, loss = 0.716247\n",
      "iteration #      40, loss = 0.715969\n",
      "iteration #      41, loss = 0.715692\n",
      "iteration #      42, loss = 0.715415\n",
      "iteration #      43, loss = 0.715138\n",
      "iteration #      44, loss = 0.714860\n",
      "iteration #      45, loss = 0.714583\n",
      "iteration #      46, loss = 0.714306\n",
      "iteration #      47, loss = 0.714028\n",
      "iteration #      48, loss = 0.713751\n",
      "iteration #      49, loss = 0.713473\n",
      "iteration #      50, loss = 0.713196\n",
      "iteration #      51, loss = 0.712918\n",
      "iteration #      52, loss = 0.712641\n",
      "iteration #      53, loss = 0.712363\n",
      "iteration #      54, loss = 0.712085\n",
      "iteration #      55, loss = 0.711807\n",
      "iteration #      56, loss = 0.711530\n",
      "iteration #      57, loss = 0.711252\n",
      "iteration #      58, loss = 0.710974\n",
      "iteration #      59, loss = 0.710696\n",
      "iteration #      60, loss = 0.710418\n",
      "iteration #      61, loss = 0.710140\n",
      "iteration #      62, loss = 0.709863\n",
      "iteration #      63, loss = 0.709585\n",
      "iteration #      64, loss = 0.709307\n",
      "iteration #      65, loss = 0.709029\n",
      "iteration #      66, loss = 0.708750\n",
      "iteration #      67, loss = 0.708472\n",
      "iteration #      68, loss = 0.708194\n",
      "iteration #      69, loss = 0.707916\n",
      "iteration #      70, loss = 0.707638\n",
      "iteration #      71, loss = 0.707360\n",
      "iteration #      72, loss = 0.707081\n",
      "iteration #      73, loss = 0.706803\n",
      "iteration #      74, loss = 0.706525\n",
      "iteration #      75, loss = 0.706246\n",
      "iteration #      76, loss = 0.705968\n",
      "iteration #      77, loss = 0.705690\n",
      "iteration #      78, loss = 0.705411\n",
      "iteration #      79, loss = 0.705133\n",
      "iteration #      80, loss = 0.704854\n",
      "iteration #      81, loss = 0.704576\n",
      "iteration #      82, loss = 0.704297\n",
      "iteration #      83, loss = 0.704019\n",
      "iteration #      84, loss = 0.703740\n",
      "iteration #      85, loss = 0.703462\n",
      "iteration #      86, loss = 0.703183\n",
      "iteration #      87, loss = 0.702905\n",
      "iteration #      88, loss = 0.702626\n",
      "iteration #      89, loss = 0.702347\n",
      "iteration #      90, loss = 0.702069\n",
      "iteration #      91, loss = 0.701790\n",
      "iteration #      92, loss = 0.701511\n",
      "iteration #      93, loss = 0.701232\n",
      "iteration #      94, loss = 0.700954\n",
      "iteration #      95, loss = 0.700675\n",
      "iteration #      96, loss = 0.700396\n",
      "iteration #      97, loss = 0.700117\n",
      "iteration #      98, loss = 0.699838\n",
      "iteration #      99, loss = 0.699560\n",
      "iteration #     100, loss = 0.699281\n",
      "iteration #     101, loss = 0.699002\n",
      "iteration #     102, loss = 0.698723\n",
      "iteration #     103, loss = 0.698444\n",
      "iteration #     104, loss = 0.698165\n",
      "iteration #     105, loss = 0.697886\n",
      "iteration #     106, loss = 0.697607\n",
      "iteration #     107, loss = 0.697328\n",
      "iteration #     108, loss = 0.697049\n",
      "iteration #     109, loss = 0.696770\n",
      "iteration #     110, loss = 0.696491\n",
      "iteration #     111, loss = 0.696212\n",
      "iteration #     112, loss = 0.695933\n",
      "iteration #     113, loss = 0.695654\n",
      "iteration #     114, loss = 0.695375\n",
      "iteration #     115, loss = 0.695096\n",
      "iteration #     116, loss = 0.694817\n",
      "iteration #     117, loss = 0.694538\n",
      "iteration #     118, loss = 0.694259\n",
      "iteration #     119, loss = 0.693980\n",
      "iteration #     120, loss = 0.693700\n",
      "iteration #     121, loss = 0.693421\n",
      "iteration #     122, loss = 0.693142\n",
      "iteration #     123, loss = 0.692863\n",
      "iteration #     124, loss = 0.692584\n",
      "iteration #     125, loss = 0.692305\n",
      "iteration #     126, loss = 0.692026\n",
      "iteration #     127, loss = 0.691746\n",
      "iteration #     128, loss = 0.691467\n",
      "iteration #     129, loss = 0.691188\n",
      "iteration #     130, loss = 0.690909\n",
      "iteration #     131, loss = 0.690630\n",
      "iteration #     132, loss = 0.690350\n",
      "iteration #     133, loss = 0.690071\n",
      "iteration #     134, loss = 0.689792\n",
      "iteration #     135, loss = 0.689513\n",
      "iteration #     136, loss = 0.689234\n",
      "iteration #     137, loss = 0.688955\n",
      "iteration #     138, loss = 0.688675\n",
      "iteration #     139, loss = 0.688396\n",
      "iteration #     140, loss = 0.688117\n",
      "iteration #     141, loss = 0.687838\n",
      "iteration #     142, loss = 0.687559\n",
      "iteration #     143, loss = 0.687279\n",
      "iteration #     144, loss = 0.687000\n",
      "iteration #     145, loss = 0.686721\n",
      "iteration #     146, loss = 0.686442\n",
      "iteration #     147, loss = 0.686163\n",
      "iteration #     148, loss = 0.685883\n",
      "iteration #     149, loss = 0.685604\n",
      "iteration #     150, loss = 0.685325\n",
      "iteration #     151, loss = 0.685046\n",
      "iteration #     152, loss = 0.684767\n",
      "iteration #     153, loss = 0.684487\n",
      "iteration #     154, loss = 0.684208\n",
      "iteration #     155, loss = 0.683929\n",
      "iteration #     156, loss = 0.683650\n",
      "iteration #     157, loss = 0.683371\n",
      "iteration #     158, loss = 0.683092\n",
      "iteration #     159, loss = 0.682813\n",
      "iteration #     160, loss = 0.682534\n",
      "iteration #     161, loss = 0.682254\n",
      "iteration #     162, loss = 0.681975\n",
      "iteration #     163, loss = 0.681696\n",
      "iteration #     164, loss = 0.681417\n",
      "iteration #     165, loss = 0.681138\n",
      "iteration #     166, loss = 0.680859\n",
      "iteration #     167, loss = 0.680580\n",
      "iteration #     168, loss = 0.680301\n",
      "iteration #     169, loss = 0.680022\n",
      "iteration #     170, loss = 0.679743\n",
      "iteration #     171, loss = 0.679464\n",
      "iteration #     172, loss = 0.679185\n",
      "iteration #     173, loss = 0.678906\n",
      "iteration #     174, loss = 0.678627\n",
      "iteration #     175, loss = 0.678348\n",
      "iteration #     176, loss = 0.678069\n",
      "iteration #     177, loss = 0.677790\n",
      "iteration #     178, loss = 0.677511\n",
      "iteration #     179, loss = 0.677232\n",
      "iteration #     180, loss = 0.676954\n",
      "iteration #     181, loss = 0.676675\n",
      "iteration #     182, loss = 0.676396\n",
      "iteration #     183, loss = 0.676117\n",
      "iteration #     184, loss = 0.675838\n",
      "iteration #     185, loss = 0.675560\n",
      "iteration #     186, loss = 0.675281\n",
      "iteration #     187, loss = 0.675002\n",
      "iteration #     188, loss = 0.674723\n",
      "iteration #     189, loss = 0.674445\n",
      "iteration #     190, loss = 0.674166\n",
      "iteration #     191, loss = 0.673887\n",
      "iteration #     192, loss = 0.673609\n",
      "iteration #     193, loss = 0.673330\n",
      "iteration #     194, loss = 0.673051\n",
      "iteration #     195, loss = 0.672773\n",
      "iteration #     196, loss = 0.672494\n",
      "iteration #     197, loss = 0.672216\n",
      "iteration #     198, loss = 0.671937\n",
      "iteration #     199, loss = 0.671659\n",
      "iteration #     200, loss = 0.671380\n",
      "iteration #     201, loss = 0.671102\n",
      "iteration #     202, loss = 0.670824\n",
      "iteration #     203, loss = 0.670545\n",
      "iteration #     204, loss = 0.670267\n",
      "iteration #     205, loss = 0.669989\n",
      "iteration #     206, loss = 0.669710\n",
      "iteration #     207, loss = 0.669432\n",
      "iteration #     208, loss = 0.669154\n",
      "iteration #     209, loss = 0.668876\n",
      "iteration #     210, loss = 0.668597\n",
      "iteration #     211, loss = 0.668319\n",
      "iteration #     212, loss = 0.668041\n",
      "iteration #     213, loss = 0.667763\n",
      "iteration #     214, loss = 0.667485\n",
      "iteration #     215, loss = 0.667207\n",
      "iteration #     216, loss = 0.666929\n",
      "iteration #     217, loss = 0.666651\n",
      "iteration #     218, loss = 0.666373\n",
      "iteration #     219, loss = 0.666095\n",
      "iteration #     220, loss = 0.665817\n",
      "iteration #     221, loss = 0.665540\n",
      "iteration #     222, loss = 0.665262\n",
      "iteration #     223, loss = 0.664984\n",
      "iteration #     224, loss = 0.664706\n",
      "iteration #     225, loss = 0.664429\n",
      "iteration #     226, loss = 0.664151\n",
      "iteration #     227, loss = 0.663873\n",
      "iteration #     228, loss = 0.663596\n",
      "iteration #     229, loss = 0.663318\n",
      "iteration #     230, loss = 0.663041\n",
      "iteration #     231, loss = 0.662763\n",
      "iteration #     232, loss = 0.662486\n",
      "iteration #     233, loss = 0.662208\n",
      "iteration #     234, loss = 0.661931\n",
      "iteration #     235, loss = 0.661654\n",
      "iteration #     236, loss = 0.661376\n",
      "iteration #     237, loss = 0.661099\n",
      "iteration #     238, loss = 0.660822\n",
      "iteration #     239, loss = 0.660545\n",
      "iteration #     240, loss = 0.660268\n",
      "iteration #     241, loss = 0.659991\n",
      "iteration #     242, loss = 0.659714\n",
      "iteration #     243, loss = 0.659437\n",
      "iteration #     244, loss = 0.659160\n",
      "iteration #     245, loss = 0.658883\n",
      "iteration #     246, loss = 0.658606\n",
      "iteration #     247, loss = 0.658329\n",
      "iteration #     248, loss = 0.658052\n",
      "iteration #     249, loss = 0.657776\n",
      "iteration #     250, loss = 0.657499\n",
      "iteration #     251, loss = 0.657222\n",
      "iteration #     252, loss = 0.656946\n",
      "iteration #     253, loss = 0.656669\n",
      "iteration #     254, loss = 0.656393\n",
      "iteration #     255, loss = 0.656116\n",
      "iteration #     256, loss = 0.655840\n",
      "iteration #     257, loss = 0.655563\n",
      "iteration #     258, loss = 0.655287\n",
      "iteration #     259, loss = 0.655011\n",
      "iteration #     260, loss = 0.654735\n",
      "iteration #     261, loss = 0.654458\n",
      "iteration #     262, loss = 0.654182\n",
      "iteration #     263, loss = 0.653906\n",
      "iteration #     264, loss = 0.653630\n",
      "iteration #     265, loss = 0.653354\n",
      "iteration #     266, loss = 0.653078\n",
      "iteration #     267, loss = 0.652802\n",
      "iteration #     268, loss = 0.652527\n",
      "iteration #     269, loss = 0.652251\n",
      "iteration #     270, loss = 0.651975\n",
      "iteration #     271, loss = 0.651699\n",
      "iteration #     272, loss = 0.651424\n",
      "iteration #     273, loss = 0.651148\n",
      "iteration #     274, loss = 0.650873\n",
      "iteration #     275, loss = 0.650597\n",
      "iteration #     276, loss = 0.650322\n",
      "iteration #     277, loss = 0.650047\n",
      "iteration #     278, loss = 0.649771\n",
      "iteration #     279, loss = 0.649496\n",
      "iteration #     280, loss = 0.649221\n",
      "iteration #     281, loss = 0.648946\n",
      "iteration #     282, loss = 0.648671\n",
      "iteration #     283, loss = 0.648396\n",
      "iteration #     284, loss = 0.648121\n",
      "iteration #     285, loss = 0.647846\n",
      "iteration #     286, loss = 0.647571\n",
      "iteration #     287, loss = 0.647296\n",
      "iteration #     288, loss = 0.647022\n",
      "iteration #     289, loss = 0.646747\n",
      "iteration #     290, loss = 0.646473\n",
      "iteration #     291, loss = 0.646198\n",
      "iteration #     292, loss = 0.645924\n",
      "iteration #     293, loss = 0.645649\n",
      "iteration #     294, loss = 0.645375\n",
      "iteration #     295, loss = 0.645101\n",
      "iteration #     296, loss = 0.644826\n",
      "iteration #     297, loss = 0.644552\n",
      "iteration #     298, loss = 0.644278\n",
      "iteration #     299, loss = 0.644004\n",
      "iteration #     300, loss = 0.643730\n",
      "iteration #     301, loss = 0.643456\n",
      "iteration #     302, loss = 0.643183\n",
      "iteration #     303, loss = 0.642909\n",
      "iteration #     304, loss = 0.642635\n",
      "iteration #     305, loss = 0.642361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     306, loss = 0.642088\n",
      "iteration #     307, loss = 0.641814\n",
      "iteration #     308, loss = 0.641541\n",
      "iteration #     309, loss = 0.641268\n",
      "iteration #     310, loss = 0.640994\n",
      "iteration #     311, loss = 0.640721\n",
      "iteration #     312, loss = 0.640448\n",
      "iteration #     313, loss = 0.640175\n",
      "iteration #     314, loss = 0.639902\n",
      "iteration #     315, loss = 0.639629\n",
      "iteration #     316, loss = 0.639356\n",
      "iteration #     317, loss = 0.639083\n",
      "iteration #     318, loss = 0.638811\n",
      "iteration #     319, loss = 0.638538\n",
      "iteration #     320, loss = 0.638265\n",
      "iteration #     321, loss = 0.637993\n",
      "iteration #     322, loss = 0.637720\n",
      "iteration #     323, loss = 0.637448\n",
      "iteration #     324, loss = 0.637176\n",
      "iteration #     325, loss = 0.636903\n",
      "iteration #     326, loss = 0.636631\n",
      "iteration #     327, loss = 0.636359\n",
      "iteration #     328, loss = 0.636087\n",
      "iteration #     329, loss = 0.635815\n",
      "iteration #     330, loss = 0.635543\n",
      "iteration #     331, loss = 0.635272\n",
      "iteration #     332, loss = 0.635000\n",
      "iteration #     333, loss = 0.634728\n",
      "iteration #     334, loss = 0.634457\n",
      "iteration #     335, loss = 0.634185\n",
      "iteration #     336, loss = 0.633914\n",
      "iteration #     337, loss = 0.633643\n",
      "iteration #     338, loss = 0.633371\n",
      "iteration #     339, loss = 0.633100\n",
      "iteration #     340, loss = 0.632829\n",
      "iteration #     341, loss = 0.632558\n",
      "iteration #     342, loss = 0.632287\n",
      "iteration #     343, loss = 0.632016\n",
      "iteration #     344, loss = 0.631746\n",
      "iteration #     345, loss = 0.631475\n",
      "iteration #     346, loss = 0.631204\n",
      "iteration #     347, loss = 0.630934\n",
      "iteration #     348, loss = 0.630663\n",
      "iteration #     349, loss = 0.630393\n",
      "iteration #     350, loss = 0.630123\n",
      "iteration #     351, loss = 0.629852\n",
      "iteration #     352, loss = 0.629582\n",
      "iteration #     353, loss = 0.629312\n",
      "iteration #     354, loss = 0.629042\n",
      "iteration #     355, loss = 0.628773\n",
      "iteration #     356, loss = 0.628503\n",
      "iteration #     357, loss = 0.628233\n",
      "iteration #     358, loss = 0.627964\n",
      "iteration #     359, loss = 0.627694\n",
      "iteration #     360, loss = 0.627425\n",
      "iteration #     361, loss = 0.627155\n",
      "iteration #     362, loss = 0.626886\n",
      "iteration #     363, loss = 0.626617\n",
      "iteration #     364, loss = 0.626348\n",
      "iteration #     365, loss = 0.626079\n",
      "iteration #     366, loss = 0.625810\n",
      "iteration #     367, loss = 0.625541\n",
      "iteration #     368, loss = 0.625272\n",
      "iteration #     369, loss = 0.625004\n",
      "iteration #     370, loss = 0.624735\n",
      "iteration #     371, loss = 0.624467\n",
      "iteration #     372, loss = 0.624198\n",
      "iteration #     373, loss = 0.623930\n",
      "iteration #     374, loss = 0.623662\n",
      "iteration #     375, loss = 0.623394\n",
      "iteration #     376, loss = 0.623126\n",
      "iteration #     377, loss = 0.622858\n",
      "iteration #     378, loss = 0.622590\n",
      "iteration #     379, loss = 0.622322\n",
      "iteration #     380, loss = 0.622055\n",
      "iteration #     381, loss = 0.621787\n",
      "iteration #     382, loss = 0.621520\n",
      "iteration #     383, loss = 0.621253\n",
      "iteration #     384, loss = 0.620985\n",
      "iteration #     385, loss = 0.620718\n",
      "iteration #     386, loss = 0.620451\n",
      "iteration #     387, loss = 0.620184\n",
      "iteration #     388, loss = 0.619917\n",
      "iteration #     389, loss = 0.619651\n",
      "iteration #     390, loss = 0.619384\n",
      "iteration #     391, loss = 0.619117\n",
      "iteration #     392, loss = 0.618851\n",
      "iteration #     393, loss = 0.618585\n",
      "iteration #     394, loss = 0.618318\n",
      "iteration #     395, loss = 0.618052\n",
      "iteration #     396, loss = 0.617786\n",
      "iteration #     397, loss = 0.617520\n",
      "iteration #     398, loss = 0.617254\n",
      "iteration #     399, loss = 0.616988\n",
      "iteration #     400, loss = 0.616723\n",
      "iteration #     401, loss = 0.616457\n",
      "iteration #     402, loss = 0.616192\n",
      "iteration #     403, loss = 0.615926\n",
      "iteration #     404, loss = 0.615661\n",
      "iteration #     405, loss = 0.615396\n",
      "iteration #     406, loss = 0.615131\n",
      "iteration #     407, loss = 0.614866\n",
      "iteration #     408, loss = 0.614601\n",
      "iteration #     409, loss = 0.614336\n",
      "iteration #     410, loss = 0.614072\n",
      "iteration #     411, loss = 0.613807\n",
      "iteration #     412, loss = 0.613543\n",
      "iteration #     413, loss = 0.613278\n",
      "iteration #     414, loss = 0.613014\n",
      "iteration #     415, loss = 0.612750\n",
      "iteration #     416, loss = 0.612486\n",
      "iteration #     417, loss = 0.612222\n",
      "iteration #     418, loss = 0.611958\n",
      "iteration #     419, loss = 0.611695\n",
      "iteration #     420, loss = 0.611431\n",
      "iteration #     421, loss = 0.611167\n",
      "iteration #     422, loss = 0.610904\n",
      "iteration #     423, loss = 0.610641\n",
      "iteration #     424, loss = 0.610378\n",
      "iteration #     425, loss = 0.610115\n",
      "iteration #     426, loss = 0.609852\n",
      "iteration #     427, loss = 0.609589\n",
      "iteration #     428, loss = 0.609326\n",
      "iteration #     429, loss = 0.609064\n",
      "iteration #     430, loss = 0.608801\n",
      "iteration #     431, loss = 0.608539\n",
      "iteration #     432, loss = 0.608276\n",
      "iteration #     433, loss = 0.608014\n",
      "iteration #     434, loss = 0.607752\n",
      "iteration #     435, loss = 0.607490\n",
      "iteration #     436, loss = 0.607229\n",
      "iteration #     437, loss = 0.606967\n",
      "iteration #     438, loss = 0.606705\n",
      "iteration #     439, loss = 0.606444\n",
      "iteration #     440, loss = 0.606182\n",
      "iteration #     441, loss = 0.605921\n",
      "iteration #     442, loss = 0.605660\n",
      "iteration #     443, loss = 0.605399\n",
      "iteration #     444, loss = 0.605138\n",
      "iteration #     445, loss = 0.604877\n",
      "iteration #     446, loss = 0.604617\n",
      "iteration #     447, loss = 0.604356\n",
      "iteration #     448, loss = 0.604096\n",
      "iteration #     449, loss = 0.603835\n",
      "iteration #     450, loss = 0.603575\n",
      "iteration #     451, loss = 0.603315\n",
      "iteration #     452, loss = 0.603055\n",
      "iteration #     453, loss = 0.602795\n",
      "iteration #     454, loss = 0.602536\n",
      "iteration #     455, loss = 0.602276\n",
      "iteration #     456, loss = 0.602016\n",
      "iteration #     457, loss = 0.601757\n",
      "iteration #     458, loss = 0.601498\n",
      "iteration #     459, loss = 0.601239\n",
      "iteration #     460, loss = 0.600980\n",
      "iteration #     461, loss = 0.600721\n",
      "iteration #     462, loss = 0.600462\n",
      "iteration #     463, loss = 0.600203\n",
      "iteration #     464, loss = 0.599945\n",
      "iteration #     465, loss = 0.599686\n",
      "iteration #     466, loss = 0.599428\n",
      "iteration #     467, loss = 0.599170\n",
      "iteration #     468, loss = 0.598912\n",
      "iteration #     469, loss = 0.598654\n",
      "iteration #     470, loss = 0.598396\n",
      "iteration #     471, loss = 0.598139\n",
      "iteration #     472, loss = 0.597881\n",
      "iteration #     473, loss = 0.597624\n",
      "iteration #     474, loss = 0.597366\n",
      "iteration #     475, loss = 0.597109\n",
      "iteration #     476, loss = 0.596852\n",
      "iteration #     477, loss = 0.596595\n",
      "iteration #     478, loss = 0.596339\n",
      "iteration #     479, loss = 0.596082\n",
      "iteration #     480, loss = 0.595825\n",
      "iteration #     481, loss = 0.595569\n",
      "iteration #     482, loss = 0.595313\n",
      "iteration #     483, loss = 0.595057\n",
      "iteration #     484, loss = 0.594801\n",
      "iteration #     485, loss = 0.594545\n",
      "iteration #     486, loss = 0.594289\n",
      "iteration #     487, loss = 0.594033\n",
      "iteration #     488, loss = 0.593778\n",
      "iteration #     489, loss = 0.593522\n",
      "iteration #     490, loss = 0.593267\n",
      "iteration #     491, loss = 0.593012\n",
      "iteration #     492, loss = 0.592757\n",
      "iteration #     493, loss = 0.592502\n",
      "iteration #     494, loss = 0.592247\n",
      "iteration #     495, loss = 0.591993\n",
      "iteration #     496, loss = 0.591738\n",
      "iteration #     497, loss = 0.591484\n",
      "iteration #     498, loss = 0.591230\n",
      "iteration #     499, loss = 0.590976\n",
      "iteration #     500, loss = 0.590722\n",
      "iteration #     501, loss = 0.590468\n",
      "iteration #     502, loss = 0.590214\n",
      "iteration #     503, loss = 0.589961\n",
      "iteration #     504, loss = 0.589707\n",
      "iteration #     505, loss = 0.589454\n",
      "iteration #     506, loss = 0.589201\n",
      "iteration #     507, loss = 0.588948\n",
      "iteration #     508, loss = 0.588695\n",
      "iteration #     509, loss = 0.588443\n",
      "iteration #     510, loss = 0.588190\n",
      "iteration #     511, loss = 0.587937\n",
      "iteration #     512, loss = 0.587685\n",
      "iteration #     513, loss = 0.587433\n",
      "iteration #     514, loss = 0.587181\n",
      "iteration #     515, loss = 0.586929\n",
      "iteration #     516, loss = 0.586677\n",
      "iteration #     517, loss = 0.586426\n",
      "iteration #     518, loss = 0.586174\n",
      "iteration #     519, loss = 0.585923\n",
      "iteration #     520, loss = 0.585672\n",
      "iteration #     521, loss = 0.585420\n",
      "iteration #     522, loss = 0.585170\n",
      "iteration #     523, loss = 0.584919\n",
      "iteration #     524, loss = 0.584668\n",
      "iteration #     525, loss = 0.584417\n",
      "iteration #     526, loss = 0.584167\n",
      "iteration #     527, loss = 0.583917\n",
      "iteration #     528, loss = 0.583667\n",
      "iteration #     529, loss = 0.583417\n",
      "iteration #     530, loss = 0.583167\n",
      "iteration #     531, loss = 0.582917\n",
      "iteration #     532, loss = 0.582668\n",
      "iteration #     533, loss = 0.582418\n",
      "iteration #     534, loss = 0.582169\n",
      "iteration #     535, loss = 0.581920\n",
      "iteration #     536, loss = 0.581671\n",
      "iteration #     537, loss = 0.581422\n",
      "iteration #     538, loss = 0.581173\n",
      "iteration #     539, loss = 0.580925\n",
      "iteration #     540, loss = 0.580676\n",
      "iteration #     541, loss = 0.580428\n",
      "iteration #     542, loss = 0.580180\n",
      "iteration #     543, loss = 0.579932\n",
      "iteration #     544, loss = 0.579684\n",
      "iteration #     545, loss = 0.579437\n",
      "iteration #     546, loss = 0.579189\n",
      "iteration #     547, loss = 0.578942\n",
      "iteration #     548, loss = 0.578694\n",
      "iteration #     549, loss = 0.578447\n",
      "iteration #     550, loss = 0.578200\n",
      "iteration #     551, loss = 0.577953\n",
      "iteration #     552, loss = 0.577707\n",
      "iteration #     553, loss = 0.577460\n",
      "iteration #     554, loss = 0.577214\n",
      "iteration #     555, loss = 0.576968\n",
      "iteration #     556, loss = 0.576721\n",
      "iteration #     557, loss = 0.576476\n",
      "iteration #     558, loss = 0.576230\n",
      "iteration #     559, loss = 0.575984\n",
      "iteration #     560, loss = 0.575739\n",
      "iteration #     561, loss = 0.575493\n",
      "iteration #     562, loss = 0.575248\n",
      "iteration #     563, loss = 0.575003\n",
      "iteration #     564, loss = 0.574758\n",
      "iteration #     565, loss = 0.574513\n",
      "iteration #     566, loss = 0.574269\n",
      "iteration #     567, loss = 0.574024\n",
      "iteration #     568, loss = 0.573780\n",
      "iteration #     569, loss = 0.573536\n",
      "iteration #     570, loss = 0.573292\n",
      "iteration #     571, loss = 0.573048\n",
      "iteration #     572, loss = 0.572804\n",
      "iteration #     573, loss = 0.572560\n",
      "iteration #     574, loss = 0.572317\n",
      "iteration #     575, loss = 0.572074\n",
      "iteration #     576, loss = 0.571831\n",
      "iteration #     577, loss = 0.571588\n",
      "iteration #     578, loss = 0.571345\n",
      "iteration #     579, loss = 0.571102\n",
      "iteration #     580, loss = 0.570860\n",
      "iteration #     581, loss = 0.570617\n",
      "iteration #     582, loss = 0.570375\n",
      "iteration #     583, loss = 0.570133\n",
      "iteration #     584, loss = 0.569891\n",
      "iteration #     585, loss = 0.569649\n",
      "iteration #     586, loss = 0.569408\n",
      "iteration #     587, loss = 0.569166\n",
      "iteration #     588, loss = 0.568925\n",
      "iteration #     589, loss = 0.568684\n",
      "iteration #     590, loss = 0.568443\n",
      "iteration #     591, loss = 0.568202\n",
      "iteration #     592, loss = 0.567961\n",
      "iteration #     593, loss = 0.567721\n",
      "iteration #     594, loss = 0.567480\n",
      "iteration #     595, loss = 0.567240\n",
      "iteration #     596, loss = 0.567000\n",
      "iteration #     597, loss = 0.566760\n",
      "iteration #     598, loss = 0.566520\n",
      "iteration #     599, loss = 0.566281\n",
      "iteration #     600, loss = 0.566041\n",
      "iteration #     601, loss = 0.565802\n",
      "iteration #     602, loss = 0.565563\n",
      "iteration #     603, loss = 0.565324\n",
      "iteration #     604, loss = 0.565085\n",
      "iteration #     605, loss = 0.564846\n",
      "iteration #     606, loss = 0.564608\n",
      "iteration #     607, loss = 0.564369\n",
      "iteration #     608, loss = 0.564131\n",
      "iteration #     609, loss = 0.563893\n",
      "iteration #     610, loss = 0.563655\n",
      "iteration #     611, loss = 0.563418\n",
      "iteration #     612, loss = 0.563180\n",
      "iteration #     613, loss = 0.562942\n",
      "iteration #     614, loss = 0.562705\n",
      "iteration #     615, loss = 0.562468\n",
      "iteration #     616, loss = 0.562231\n",
      "iteration #     617, loss = 0.561994\n",
      "iteration #     618, loss = 0.561758\n",
      "iteration #     619, loss = 0.561521\n",
      "iteration #     620, loss = 0.561285\n",
      "iteration #     621, loss = 0.561049\n",
      "iteration #     622, loss = 0.560813\n",
      "iteration #     623, loss = 0.560577\n",
      "iteration #     624, loss = 0.560341\n",
      "iteration #     625, loss = 0.560106\n",
      "iteration #     626, loss = 0.559870\n",
      "iteration #     627, loss = 0.559635\n",
      "iteration #     628, loss = 0.559400\n",
      "iteration #     629, loss = 0.559165\n",
      "iteration #     630, loss = 0.558930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     631, loss = 0.558696\n",
      "iteration #     632, loss = 0.558461\n",
      "iteration #     633, loss = 0.558227\n",
      "iteration #     634, loss = 0.557993\n",
      "iteration #     635, loss = 0.557759\n",
      "iteration #     636, loss = 0.557525\n",
      "iteration #     637, loss = 0.557292\n",
      "iteration #     638, loss = 0.557058\n",
      "iteration #     639, loss = 0.556825\n",
      "iteration #     640, loss = 0.556592\n",
      "iteration #     641, loss = 0.556359\n",
      "iteration #     642, loss = 0.556126\n",
      "iteration #     643, loss = 0.555893\n",
      "iteration #     644, loss = 0.555661\n",
      "iteration #     645, loss = 0.555428\n",
      "iteration #     646, loss = 0.555196\n",
      "iteration #     647, loss = 0.554964\n",
      "iteration #     648, loss = 0.554732\n",
      "iteration #     649, loss = 0.554501\n",
      "iteration #     650, loss = 0.554269\n",
      "iteration #     651, loss = 0.554038\n",
      "iteration #     652, loss = 0.553807\n",
      "iteration #     653, loss = 0.553576\n",
      "iteration #     654, loss = 0.553345\n",
      "iteration #     655, loss = 0.553114\n",
      "iteration #     656, loss = 0.552883\n",
      "iteration #     657, loss = 0.552653\n",
      "iteration #     658, loss = 0.552423\n",
      "iteration #     659, loss = 0.552193\n",
      "iteration #     660, loss = 0.551963\n",
      "iteration #     661, loss = 0.551733\n",
      "iteration #     662, loss = 0.551504\n",
      "iteration #     663, loss = 0.551274\n",
      "iteration #     664, loss = 0.551045\n",
      "iteration #     665, loss = 0.550816\n",
      "iteration #     666, loss = 0.550587\n",
      "iteration #     667, loss = 0.550358\n",
      "iteration #     668, loss = 0.550130\n",
      "iteration #     669, loss = 0.549901\n",
      "iteration #     670, loss = 0.549673\n",
      "iteration #     671, loss = 0.549445\n",
      "iteration #     672, loss = 0.549217\n",
      "iteration #     673, loss = 0.548989\n",
      "iteration #     674, loss = 0.548761\n",
      "iteration #     675, loss = 0.548534\n",
      "iteration #     676, loss = 0.548307\n",
      "iteration #     677, loss = 0.548080\n",
      "iteration #     678, loss = 0.547853\n",
      "iteration #     679, loss = 0.547626\n",
      "iteration #     680, loss = 0.547399\n",
      "iteration #     681, loss = 0.547173\n",
      "iteration #     682, loss = 0.546946\n",
      "iteration #     683, loss = 0.546720\n",
      "iteration #     684, loss = 0.546494\n",
      "iteration #     685, loss = 0.546269\n",
      "iteration #     686, loss = 0.546043\n",
      "iteration #     687, loss = 0.545817\n",
      "iteration #     688, loss = 0.545592\n",
      "iteration #     689, loss = 0.545367\n",
      "iteration #     690, loss = 0.545142\n",
      "iteration #     691, loss = 0.544917\n",
      "iteration #     692, loss = 0.544693\n",
      "iteration #     693, loss = 0.544468\n",
      "iteration #     694, loss = 0.544244\n",
      "iteration #     695, loss = 0.544020\n",
      "iteration #     696, loss = 0.543796\n",
      "iteration #     697, loss = 0.543572\n",
      "iteration #     698, loss = 0.543348\n",
      "iteration #     699, loss = 0.543125\n",
      "iteration #     700, loss = 0.542901\n",
      "iteration #     701, loss = 0.542678\n",
      "iteration #     702, loss = 0.542455\n",
      "iteration #     703, loss = 0.542233\n",
      "iteration #     704, loss = 0.542010\n",
      "iteration #     705, loss = 0.541787\n",
      "iteration #     706, loss = 0.541565\n",
      "iteration #     707, loss = 0.541343\n",
      "iteration #     708, loss = 0.541121\n",
      "iteration #     709, loss = 0.540899\n",
      "iteration #     710, loss = 0.540678\n",
      "iteration #     711, loss = 0.540456\n",
      "iteration #     712, loss = 0.540235\n",
      "iteration #     713, loss = 0.540014\n",
      "iteration #     714, loss = 0.539793\n",
      "iteration #     715, loss = 0.539572\n",
      "iteration #     716, loss = 0.539351\n",
      "iteration #     717, loss = 0.539131\n",
      "iteration #     718, loss = 0.538910\n",
      "iteration #     719, loss = 0.538690\n",
      "iteration #     720, loss = 0.538470\n",
      "iteration #     721, loss = 0.538250\n",
      "iteration #     722, loss = 0.538031\n",
      "iteration #     723, loss = 0.537811\n",
      "iteration #     724, loss = 0.537592\n",
      "iteration #     725, loss = 0.537373\n",
      "iteration #     726, loss = 0.537154\n",
      "iteration #     727, loss = 0.536935\n",
      "iteration #     728, loss = 0.536717\n",
      "iteration #     729, loss = 0.536498\n",
      "iteration #     730, loss = 0.536280\n",
      "iteration #     731, loss = 0.536062\n",
      "iteration #     732, loss = 0.535844\n",
      "iteration #     733, loss = 0.535626\n",
      "iteration #     734, loss = 0.535408\n",
      "iteration #     735, loss = 0.535191\n",
      "iteration #     736, loss = 0.534974\n",
      "iteration #     737, loss = 0.534757\n",
      "iteration #     738, loss = 0.534540\n",
      "iteration #     739, loss = 0.534323\n",
      "iteration #     740, loss = 0.534106\n",
      "iteration #     741, loss = 0.533890\n",
      "iteration #     742, loss = 0.533673\n",
      "iteration #     743, loss = 0.533457\n",
      "iteration #     744, loss = 0.533241\n",
      "iteration #     745, loss = 0.533026\n",
      "iteration #     746, loss = 0.532810\n",
      "iteration #     747, loss = 0.532595\n",
      "iteration #     748, loss = 0.532379\n",
      "iteration #     749, loss = 0.532164\n",
      "iteration #     750, loss = 0.531949\n",
      "iteration #     751, loss = 0.531735\n",
      "iteration #     752, loss = 0.531520\n",
      "iteration #     753, loss = 0.531306\n",
      "iteration #     754, loss = 0.531091\n",
      "iteration #     755, loss = 0.530877\n",
      "iteration #     756, loss = 0.530663\n",
      "iteration #     757, loss = 0.530450\n",
      "iteration #     758, loss = 0.530236\n",
      "iteration #     759, loss = 0.530023\n",
      "iteration #     760, loss = 0.529810\n",
      "iteration #     761, loss = 0.529596\n",
      "iteration #     762, loss = 0.529384\n",
      "iteration #     763, loss = 0.529171\n",
      "iteration #     764, loss = 0.528958\n",
      "iteration #     765, loss = 0.528746\n",
      "iteration #     766, loss = 0.528534\n",
      "iteration #     767, loss = 0.528322\n",
      "iteration #     768, loss = 0.528110\n",
      "iteration #     769, loss = 0.527898\n",
      "iteration #     770, loss = 0.527687\n",
      "iteration #     771, loss = 0.527475\n",
      "iteration #     772, loss = 0.527264\n",
      "iteration #     773, loss = 0.527053\n",
      "iteration #     774, loss = 0.526842\n",
      "iteration #     775, loss = 0.526631\n",
      "iteration #     776, loss = 0.526421\n",
      "iteration #     777, loss = 0.526211\n",
      "iteration #     778, loss = 0.526000\n",
      "iteration #     779, loss = 0.525790\n",
      "iteration #     780, loss = 0.525581\n",
      "iteration #     781, loss = 0.525371\n",
      "iteration #     782, loss = 0.525161\n",
      "iteration #     783, loss = 0.524952\n",
      "iteration #     784, loss = 0.524743\n",
      "iteration #     785, loss = 0.524534\n",
      "iteration #     786, loss = 0.524325\n",
      "iteration #     787, loss = 0.524116\n",
      "iteration #     788, loss = 0.523908\n",
      "iteration #     789, loss = 0.523700\n",
      "iteration #     790, loss = 0.523491\n",
      "iteration #     791, loss = 0.523284\n",
      "iteration #     792, loss = 0.523076\n",
      "iteration #     793, loss = 0.522868\n",
      "iteration #     794, loss = 0.522661\n",
      "iteration #     795, loss = 0.522453\n",
      "iteration #     796, loss = 0.522246\n",
      "iteration #     797, loss = 0.522039\n",
      "iteration #     798, loss = 0.521832\n",
      "iteration #     799, loss = 0.521626\n",
      "iteration #     800, loss = 0.521419\n",
      "iteration #     801, loss = 0.521213\n",
      "iteration #     802, loss = 0.521007\n",
      "iteration #     803, loss = 0.520801\n",
      "iteration #     804, loss = 0.520595\n",
      "iteration #     805, loss = 0.520390\n",
      "iteration #     806, loss = 0.520184\n",
      "iteration #     807, loss = 0.519979\n",
      "iteration #     808, loss = 0.519774\n",
      "iteration #     809, loss = 0.519569\n",
      "iteration #     810, loss = 0.519364\n",
      "iteration #     811, loss = 0.519160\n",
      "iteration #     812, loss = 0.518955\n",
      "iteration #     813, loss = 0.518751\n",
      "iteration #     814, loss = 0.518547\n",
      "iteration #     815, loss = 0.518343\n",
      "iteration #     816, loss = 0.518139\n",
      "iteration #     817, loss = 0.517936\n",
      "iteration #     818, loss = 0.517732\n",
      "iteration #     819, loss = 0.517529\n",
      "iteration #     820, loss = 0.517326\n",
      "iteration #     821, loss = 0.517123\n",
      "iteration #     822, loss = 0.516921\n",
      "iteration #     823, loss = 0.516718\n",
      "iteration #     824, loss = 0.516516\n",
      "iteration #     825, loss = 0.516314\n",
      "iteration #     826, loss = 0.516112\n",
      "iteration #     827, loss = 0.515910\n",
      "iteration #     828, loss = 0.515708\n",
      "iteration #     829, loss = 0.515506\n",
      "iteration #     830, loss = 0.515305\n",
      "iteration #     831, loss = 0.515104\n",
      "iteration #     832, loss = 0.514903\n",
      "iteration #     833, loss = 0.514702\n",
      "iteration #     834, loss = 0.514501\n",
      "iteration #     835, loss = 0.514301\n",
      "iteration #     836, loss = 0.514101\n",
      "iteration #     837, loss = 0.513900\n",
      "iteration #     838, loss = 0.513700\n",
      "iteration #     839, loss = 0.513501\n",
      "iteration #     840, loss = 0.513301\n",
      "iteration #     841, loss = 0.513102\n",
      "iteration #     842, loss = 0.512902\n",
      "iteration #     843, loss = 0.512703\n",
      "iteration #     844, loss = 0.512504\n",
      "iteration #     845, loss = 0.512305\n",
      "iteration #     846, loss = 0.512107\n",
      "iteration #     847, loss = 0.511908\n",
      "iteration #     848, loss = 0.511710\n",
      "iteration #     849, loss = 0.511512\n",
      "iteration #     850, loss = 0.511314\n",
      "iteration #     851, loss = 0.511116\n",
      "iteration #     852, loss = 0.510918\n",
      "iteration #     853, loss = 0.510721\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(1, x_training_aug.shape[1])\n",
    "theta = bgd(x_training_aug, y_training, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the parameters are learned, we can test the performance of the classifier. Note that logistic regression returns a number in [0,1], thus we need to binarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression test error = 0.6417\n"
     ]
    }
   ],
   "source": [
    "y_prediction = (h(x_testing_aug, theta)>=0.5)[:,0]\n",
    "test_error = np.sum(y_testing != y_prediction) / y_testing.shape[0]\n",
    "print('logistic regression test error = {:.4f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
