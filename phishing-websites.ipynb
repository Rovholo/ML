{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Websites\n",
    "\n",
    "\n",
    "## About the Dataset\n",
    "This dataset consists of 11055 training data set and 2456 testing data set. It has 30 attributes, namely, \n",
    "- having_IP_Address  { -1,1 }\n",
    "- URL_Length   { 1,0,-1 } \n",
    "- Shortining_Service { 1,-1 } \n",
    "- having_At_Symbol   { 1,-1 } \n",
    "- double_slash_redirecting { -1,1 } \n",
    "- Prefix_Suffix  { -1,1 } \n",
    "- having_Sub_Domain  { -1,0,1 } \n",
    "- SSLfinal_State  { -1,1,0 } \n",
    "- Domain_registeration_length { -1,1 } \n",
    "- Favicon { 1,-1 } \n",
    "- port { 1,-1 } \n",
    "- HTTPS_token { -1,1 } \n",
    "- Request_URL  { 1,-1 } \n",
    "- URL_of_Anchor { -1,0,1 } \n",
    "- Links_in_tags { 1,-1,0 } \n",
    "- SFH  { -1,1,0 } \n",
    "- Submitting_to_email { -1,1 } \n",
    "- Abnormal_URL { -1,1 }\n",
    "- Redirect  { 0,1 } \n",
    "- on_mouseover  { 1,-1 }\n",
    "- RightClick  { 1,-1 } \n",
    "- popUpWidnow  { 1,-1 } \n",
    "- Iframe { 1,-1 } \n",
    "- age_of_domain  { -1,1 } \n",
    "- DNSRecord   { -1,1 } \n",
    "- web_traffic  { -1,0,1 } \n",
    "- Page_Rank { -1,1 } \n",
    "- Google_Index { 1,-1 } \n",
    "- Links_pointing_to_page { 1,0,-1 } \n",
    "- Statistical_report { -1,1 } \n",
    "- Result  { -1,1 } \n",
    "\n",
    "which they take on the values 1, 0, -1 that mean whether the website is either legitimate, suspicious or phishing, respectively.\n",
    "\n",
    "###### The following segment of code just creates and duplicates the dataset in the .arff files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Getting all the arff files from the current directory\n",
    "files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
    "\n",
    "# Function for converting arff list to csv list\n",
    "def toCsv(content):\n",
    "    data = False\n",
    "    header = \"\"\n",
    "    newContent = []\n",
    "    for line in content:\n",
    "        if not data:\n",
    "            if \"@attribute\" in line:\n",
    "                attri = line.split()\n",
    "                columnName = attri[attri.index(\"@attribute\")+1]\n",
    "                header = header + columnName + \",\"\n",
    "            elif \"@data\" in line:\n",
    "                data = True\n",
    "                header = header[:-1]\n",
    "                header += '\\n'\n",
    "                newContent.append(header)\n",
    "        else:\n",
    "            newContent.append(line)\n",
    "    return newContent\n",
    "\n",
    "# Main loop for reading and writing files\n",
    "for file in files:\n",
    "    with open(file , \"r\") as inFile:\n",
    "        content = inFile.readlines()\n",
    "        name,ext = os.path.splitext(inFile.name)\n",
    "        new = toCsv(content)\n",
    "        with open(name+\".csv\", \"w\") as outFile:\n",
    "            outFile.writelines(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset : \n",
      "number of samples_______ = 11055\n",
      "number of negative ones_ = 4898\n",
      "number of ones__________ = 6157\n",
      "testing dataset : \n",
      "number of samples_______ = 2456\n",
      "number of negative ones_ = 1362\n",
      "number of ones__________ = 1094\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"white\")\n",
    "sns.set(style = \"whitegrid\", color_codes = True)\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('Training Dataset.csv', sep = ',')\n",
    "data_test = pd.read_csv('old.csv', sep = ',')\n",
    "\n",
    "feature_index_names = {0:'having_IP_Address', 1:'URL_Length', 2:'Shortining_Service', 3:'having_At_Symbol', \n",
    "                        4:'double_slash_redirecting', 5:'Prefix_Suffix', 6:'having_Sub_Domain', 7:'SSLfinal_State',\n",
    "                        8:'Domain_registeration_length', 9:'Favicon', 10:'port', 11:'HTTPS_token',12:'Request_URL', \n",
    "                        13:'URL_of_Anchor', 14:'Links_in_tags', 15:'SFH', 16:'Submitting_to_email', 17:'Abnormal_URL', \n",
    "                        18:'Redirect', 19:'on_mouseover', 20:'RightClick', 21:'popUpWidnow', 22:'Iframe',\n",
    "                        23:' age_of_domain', 24:'DNSRecord', 25:'web_traffic', 26:'Page_Rank', 27:' Google_Index', \n",
    "                        28:'Links_pointing_to_page', 29:'Statistical_report'} \n",
    "#training dataset\n",
    "training_data = np.array(data_train)\n",
    "x_training = training_data[:, :-1]\n",
    "y_training = training_data[:, -1]\n",
    "\n",
    "print('training dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_training.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_training == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_training == 1)))\n",
    "\n",
    "#testing dataset\n",
    "testing_data = np.array(data_test)\n",
    "x_testing = testing_data[:, :-1]\n",
    "y_testing = testing_data[:, -1]\n",
    "\n",
    "print('testing dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_testing.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_testing == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_testing == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "We can also use logistic regression to perform occupancy detection. In order to achieve this, first we need to define our hypothesis (or model):\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "h_{\\boldsymbol{\\Theta}}(\\mathbf{x}) & = & \\frac{1}{1 + e^{-(\\theta_{0} x_{0} + \\theta_{1} x_{1} + \\cdots + \\theta_{d} x_{d})}} \\\\\n",
    "& = & \\frac{1}{1 + e^{-\\mathbf{x} \\boldsymbol{\\Theta}^{T}}},\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\mathbf{x} = [x_{0}, x_{1}, \\ldots, x_{d}]$, $\\boldsymbol{\\Theta}=[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{d}]$ and $x_{0} = 1$. \n",
    "\n",
    "The following function implements this model, which assumes that all vectors are row-major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, theta):\n",
    "    s = np.dot(x, theta.T)\n",
    "    u = 1.0 / (1.0 + np.exp(-s))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define our loss function to learn the parameters of the model based on training dataset. We can use mean square error as our loss function, i.e.,\n",
    "\n",
    "$$\n",
    "J\\left(\\boldsymbol{\\Theta}\\right) = \\frac{1}{2 N} \\sum\\limits_{n=1}^{N} \\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) - y^{\\left(n\\right)} \\right)^2 = \\frac{1}{N} \\sum\\limits_{n=1}^{N}Cost\\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) , y^{\\left(n\\right)}\\right).\n",
    "$$\n",
    "\n",
    "The following function implements the loss function $J\\left(\\boldsymbol{\\Theta}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, theta):\n",
    "    N = y.shape[0]\n",
    "    mse = 1.0 / (2*N) * np.sum((h(x, theta) - y)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need implement gradient descent solver to learn the parameters $\\boldsymbol{\\Theta}$ of our model. The following function implements batch gradient descent to learn the parameters $\\boldsymbol{\\Theta}$ on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(x, y, theta, alpha=0.1, epsilon=0.001, max_iter=1000):\n",
    "    y = y[:, np.newaxis]\n",
    "    N = y.shape[0]\n",
    "    t = 0\n",
    "    while True:\n",
    "        # print the value of loss function for each iteration\n",
    "        print('iteration #{:>8d}, loss = {:>8f}'.format(t, J(x,y,theta)))\n",
    "        # keep a copy of the parameter vector before the update for checking the convergence criterion\n",
    "        theta_previous = theta.copy()\n",
    "        # update the parameter vector\n",
    "        e = (h(x, theta) - y)\n",
    "        theta  = theta - alpha * 1.0 / N * np.sum( e * x, axis=0)\n",
    "        t = t + 1\n",
    "        # check the convergence criterion\n",
    "        if (np.max(np.abs(theta-theta_previous)) < epsilon) or (t>max_iter):\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(x_training, axis=0)\n",
    "s = np.std(x_training, axis=0)\n",
    "x_training = (x_training - m) / s\n",
    "x_testing = (x_testing - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment training and testing vector to take advantage of fast matrix operations in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_aug = np.hstack((np.ones((x_training.shape[0],1)), x_training))\n",
    "x_testing_aug = np.hstack((np.ones((x_testing.shape[0],1)), x_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random parameter vector and learn the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #       0, loss = 0.601014\n",
      "iteration #       1, loss = 0.584591\n",
      "iteration #       2, loss = 0.568730\n",
      "iteration #       3, loss = 0.553518\n",
      "iteration #       4, loss = 0.539009\n",
      "iteration #       5, loss = 0.525235\n",
      "iteration #       6, loss = 0.512214\n",
      "iteration #       7, loss = 0.499949\n",
      "iteration #       8, loss = 0.488437\n",
      "iteration #       9, loss = 0.477664\n",
      "iteration #      10, loss = 0.467609\n",
      "iteration #      11, loss = 0.458244\n",
      "iteration #      12, loss = 0.449539\n",
      "iteration #      13, loss = 0.441458\n",
      "iteration #      14, loss = 0.433963\n",
      "iteration #      15, loss = 0.427013\n",
      "iteration #      16, loss = 0.420568\n",
      "iteration #      17, loss = 0.414589\n",
      "iteration #      18, loss = 0.409035\n",
      "iteration #      19, loss = 0.403868\n",
      "iteration #      20, loss = 0.399049\n",
      "iteration #      21, loss = 0.394544\n",
      "iteration #      22, loss = 0.390323\n",
      "iteration #      23, loss = 0.386361\n",
      "iteration #      24, loss = 0.382634\n",
      "iteration #      25, loss = 0.379125\n",
      "iteration #      26, loss = 0.375817\n",
      "iteration #      27, loss = 0.372697\n",
      "iteration #      28, loss = 0.369752\n",
      "iteration #      29, loss = 0.366968\n",
      "iteration #      30, loss = 0.364337\n",
      "iteration #      31, loss = 0.361846\n",
      "iteration #      32, loss = 0.359487\n",
      "iteration #      33, loss = 0.357252\n",
      "iteration #      34, loss = 0.355132\n",
      "iteration #      35, loss = 0.353120\n",
      "iteration #      36, loss = 0.351209\n",
      "iteration #      37, loss = 0.349392\n",
      "iteration #      38, loss = 0.347663\n",
      "iteration #      39, loss = 0.346013\n",
      "iteration #      40, loss = 0.344438\n",
      "iteration #      41, loss = 0.342930\n",
      "iteration #      42, loss = 0.341485\n",
      "iteration #      43, loss = 0.340095\n",
      "iteration #      44, loss = 0.338757\n",
      "iteration #      45, loss = 0.337465\n",
      "iteration #      46, loss = 0.336215\n",
      "iteration #      47, loss = 0.335005\n",
      "iteration #      48, loss = 0.333830\n",
      "iteration #      49, loss = 0.332689\n",
      "iteration #      50, loss = 0.331579\n",
      "iteration #      51, loss = 0.330501\n",
      "iteration #      52, loss = 0.329455\n",
      "iteration #      53, loss = 0.328441\n",
      "iteration #      54, loss = 0.327462\n",
      "iteration #      55, loss = 0.326519\n",
      "iteration #      56, loss = 0.325613\n",
      "iteration #      57, loss = 0.324743\n",
      "iteration #      58, loss = 0.323910\n",
      "iteration #      59, loss = 0.323110\n",
      "iteration #      60, loss = 0.322343\n",
      "iteration #      61, loss = 0.321607\n",
      "iteration #      62, loss = 0.320900\n",
      "iteration #      63, loss = 0.320221\n",
      "iteration #      64, loss = 0.319567\n",
      "iteration #      65, loss = 0.318939\n",
      "iteration #      66, loss = 0.318333\n",
      "iteration #      67, loss = 0.317750\n",
      "iteration #      68, loss = 0.317188\n",
      "iteration #      69, loss = 0.316647\n",
      "iteration #      70, loss = 0.316125\n",
      "iteration #      71, loss = 0.315623\n",
      "iteration #      72, loss = 0.315139\n",
      "iteration #      73, loss = 0.314673\n",
      "iteration #      74, loss = 0.314226\n",
      "iteration #      75, loss = 0.313795\n",
      "iteration #      76, loss = 0.313382\n",
      "iteration #      77, loss = 0.312985\n",
      "iteration #      78, loss = 0.312605\n",
      "iteration #      79, loss = 0.312240\n",
      "iteration #      80, loss = 0.311891\n",
      "iteration #      81, loss = 0.311556\n",
      "iteration #      82, loss = 0.311234\n",
      "iteration #      83, loss = 0.310926\n",
      "iteration #      84, loss = 0.310630\n",
      "iteration #      85, loss = 0.310345\n",
      "iteration #      86, loss = 0.310071\n",
      "iteration #      87, loss = 0.309808\n",
      "iteration #      88, loss = 0.309554\n",
      "iteration #      89, loss = 0.309310\n",
      "iteration #      90, loss = 0.309074\n",
      "iteration #      91, loss = 0.308846\n",
      "iteration #      92, loss = 0.308625\n",
      "iteration #      93, loss = 0.308412\n",
      "iteration #      94, loss = 0.308206\n",
      "iteration #      95, loss = 0.308007\n",
      "iteration #      96, loss = 0.307814\n",
      "iteration #      97, loss = 0.307626\n",
      "iteration #      98, loss = 0.307444\n",
      "iteration #      99, loss = 0.307268\n",
      "iteration #     100, loss = 0.307096\n",
      "iteration #     101, loss = 0.306929\n",
      "iteration #     102, loss = 0.306766\n",
      "iteration #     103, loss = 0.306607\n",
      "iteration #     104, loss = 0.306453\n",
      "iteration #     105, loss = 0.306302\n",
      "iteration #     106, loss = 0.306155\n",
      "iteration #     107, loss = 0.306011\n",
      "iteration #     108, loss = 0.305871\n",
      "iteration #     109, loss = 0.305733\n",
      "iteration #     110, loss = 0.305599\n",
      "iteration #     111, loss = 0.305468\n",
      "iteration #     112, loss = 0.305340\n",
      "iteration #     113, loss = 0.305214\n",
      "iteration #     114, loss = 0.305092\n",
      "iteration #     115, loss = 0.304972\n",
      "iteration #     116, loss = 0.304854\n",
      "iteration #     117, loss = 0.304739\n",
      "iteration #     118, loss = 0.304627\n",
      "iteration #     119, loss = 0.304517\n",
      "iteration #     120, loss = 0.304410\n",
      "iteration #     121, loss = 0.304304\n",
      "iteration #     122, loss = 0.304202\n",
      "iteration #     123, loss = 0.304101\n",
      "iteration #     124, loss = 0.304003\n",
      "iteration #     125, loss = 0.303906\n",
      "iteration #     126, loss = 0.303812\n",
      "iteration #     127, loss = 0.303721\n",
      "iteration #     128, loss = 0.303631\n",
      "iteration #     129, loss = 0.303543\n",
      "iteration #     130, loss = 0.303458\n",
      "iteration #     131, loss = 0.303374\n",
      "iteration #     132, loss = 0.303292\n",
      "iteration #     133, loss = 0.303213\n",
      "iteration #     134, loss = 0.303135\n",
      "iteration #     135, loss = 0.303059\n",
      "iteration #     136, loss = 0.302985\n",
      "iteration #     137, loss = 0.302913\n",
      "iteration #     138, loss = 0.302842\n",
      "iteration #     139, loss = 0.302773\n",
      "iteration #     140, loss = 0.302706\n",
      "iteration #     141, loss = 0.302641\n",
      "iteration #     142, loss = 0.302577\n",
      "iteration #     143, loss = 0.302515\n",
      "iteration #     144, loss = 0.302454\n",
      "iteration #     145, loss = 0.302394\n",
      "iteration #     146, loss = 0.302336\n",
      "iteration #     147, loss = 0.302280\n",
      "iteration #     148, loss = 0.302225\n",
      "iteration #     149, loss = 0.302171\n",
      "iteration #     150, loss = 0.302118\n",
      "iteration #     151, loss = 0.302067\n",
      "iteration #     152, loss = 0.302017\n",
      "iteration #     153, loss = 0.301969\n",
      "iteration #     154, loss = 0.301921\n",
      "iteration #     155, loss = 0.301875\n",
      "iteration #     156, loss = 0.301830\n",
      "iteration #     157, loss = 0.301786\n",
      "iteration #     158, loss = 0.301743\n",
      "iteration #     159, loss = 0.301702\n",
      "iteration #     160, loss = 0.301662\n",
      "iteration #     161, loss = 0.301622\n",
      "iteration #     162, loss = 0.301584\n",
      "iteration #     163, loss = 0.301547\n",
      "iteration #     164, loss = 0.301511\n",
      "iteration #     165, loss = 0.301477\n",
      "iteration #     166, loss = 0.301443\n",
      "iteration #     167, loss = 0.301410\n",
      "iteration #     168, loss = 0.301379\n",
      "iteration #     169, loss = 0.301348\n",
      "iteration #     170, loss = 0.301319\n",
      "iteration #     171, loss = 0.301290\n",
      "iteration #     172, loss = 0.301263\n",
      "iteration #     173, loss = 0.301236\n",
      "iteration #     174, loss = 0.301211\n",
      "iteration #     175, loss = 0.301186\n",
      "iteration #     176, loss = 0.301163\n",
      "iteration #     177, loss = 0.301140\n",
      "iteration #     178, loss = 0.301118\n",
      "iteration #     179, loss = 0.301097\n",
      "iteration #     180, loss = 0.301077\n",
      "iteration #     181, loss = 0.301058\n",
      "iteration #     182, loss = 0.301040\n",
      "iteration #     183, loss = 0.301022\n",
      "iteration #     184, loss = 0.301006\n",
      "iteration #     185, loss = 0.300990\n",
      "iteration #     186, loss = 0.300975\n",
      "iteration #     187, loss = 0.300960\n",
      "iteration #     188, loss = 0.300946\n",
      "iteration #     189, loss = 0.300933\n",
      "iteration #     190, loss = 0.300921\n",
      "iteration #     191, loss = 0.300909\n",
      "iteration #     192, loss = 0.300898\n",
      "iteration #     193, loss = 0.300887\n",
      "iteration #     194, loss = 0.300877\n",
      "iteration #     195, loss = 0.300868\n",
      "iteration #     196, loss = 0.300859\n",
      "iteration #     197, loss = 0.300851\n",
      "iteration #     198, loss = 0.300843\n",
      "iteration #     199, loss = 0.300835\n",
      "iteration #     200, loss = 0.300828\n",
      "iteration #     201, loss = 0.300822\n",
      "iteration #     202, loss = 0.300816\n",
      "iteration #     203, loss = 0.300810\n",
      "iteration #     204, loss = 0.300805\n",
      "iteration #     205, loss = 0.300800\n",
      "iteration #     206, loss = 0.300795\n",
      "iteration #     207, loss = 0.300791\n",
      "iteration #     208, loss = 0.300788\n",
      "iteration #     209, loss = 0.300784\n",
      "iteration #     210, loss = 0.300781\n",
      "iteration #     211, loss = 0.300778\n",
      "iteration #     212, loss = 0.300776\n",
      "iteration #     213, loss = 0.300774\n",
      "iteration #     214, loss = 0.300772\n",
      "iteration #     215, loss = 0.300771\n",
      "iteration #     216, loss = 0.300770\n",
      "iteration #     217, loss = 0.300769\n",
      "iteration #     218, loss = 0.300768\n",
      "iteration #     219, loss = 0.300768\n",
      "iteration #     220, loss = 0.300768\n",
      "iteration #     221, loss = 0.300768\n",
      "iteration #     222, loss = 0.300769\n",
      "iteration #     223, loss = 0.300770\n",
      "iteration #     224, loss = 0.300771\n",
      "iteration #     225, loss = 0.300772\n",
      "iteration #     226, loss = 0.300774\n",
      "iteration #     227, loss = 0.300775\n",
      "iteration #     228, loss = 0.300777\n",
      "iteration #     229, loss = 0.300780\n",
      "iteration #     230, loss = 0.300782\n",
      "iteration #     231, loss = 0.300785\n",
      "iteration #     232, loss = 0.300788\n",
      "iteration #     233, loss = 0.300791\n",
      "iteration #     234, loss = 0.300794\n",
      "iteration #     235, loss = 0.300798\n",
      "iteration #     236, loss = 0.300801\n",
      "iteration #     237, loss = 0.300805\n",
      "iteration #     238, loss = 0.300809\n",
      "iteration #     239, loss = 0.300813\n",
      "iteration #     240, loss = 0.300818\n",
      "iteration #     241, loss = 0.300822\n",
      "iteration #     242, loss = 0.300827\n",
      "iteration #     243, loss = 0.300832\n",
      "iteration #     244, loss = 0.300837\n",
      "iteration #     245, loss = 0.300842\n",
      "iteration #     246, loss = 0.300848\n",
      "iteration #     247, loss = 0.300853\n",
      "iteration #     248, loss = 0.300859\n",
      "iteration #     249, loss = 0.300864\n",
      "iteration #     250, loss = 0.300870\n",
      "iteration #     251, loss = 0.300876\n",
      "iteration #     252, loss = 0.300882\n",
      "iteration #     253, loss = 0.300888\n",
      "iteration #     254, loss = 0.300894\n",
      "iteration #     255, loss = 0.300901\n",
      "iteration #     256, loss = 0.300907\n",
      "iteration #     257, loss = 0.300914\n",
      "iteration #     258, loss = 0.300920\n",
      "iteration #     259, loss = 0.300927\n",
      "iteration #     260, loss = 0.300934\n",
      "iteration #     261, loss = 0.300940\n",
      "iteration #     262, loss = 0.300947\n",
      "iteration #     263, loss = 0.300954\n",
      "iteration #     264, loss = 0.300961\n",
      "iteration #     265, loss = 0.300968\n",
      "iteration #     266, loss = 0.300975\n",
      "iteration #     267, loss = 0.300982\n",
      "iteration #     268, loss = 0.300989\n",
      "iteration #     269, loss = 0.300997\n",
      "iteration #     270, loss = 0.301004\n",
      "iteration #     271, loss = 0.301011\n",
      "iteration #     272, loss = 0.301018\n",
      "iteration #     273, loss = 0.301026\n",
      "iteration #     274, loss = 0.301033\n",
      "iteration #     275, loss = 0.301041\n",
      "iteration #     276, loss = 0.301048\n",
      "iteration #     277, loss = 0.301055\n",
      "iteration #     278, loss = 0.301063\n",
      "iteration #     279, loss = 0.301070\n",
      "iteration #     280, loss = 0.301078\n",
      "iteration #     281, loss = 0.301085\n",
      "iteration #     282, loss = 0.301093\n",
      "iteration #     283, loss = 0.301100\n",
      "iteration #     284, loss = 0.301108\n",
      "iteration #     285, loss = 0.301115\n",
      "iteration #     286, loss = 0.301123\n",
      "iteration #     287, loss = 0.301130\n",
      "iteration #     288, loss = 0.301138\n",
      "iteration #     289, loss = 0.301145\n",
      "iteration #     290, loss = 0.301152\n",
      "iteration #     291, loss = 0.301160\n",
      "iteration #     292, loss = 0.301167\n",
      "iteration #     293, loss = 0.301175\n",
      "iteration #     294, loss = 0.301182\n",
      "iteration #     295, loss = 0.301190\n",
      "iteration #     296, loss = 0.301197\n",
      "iteration #     297, loss = 0.301204\n",
      "iteration #     298, loss = 0.301212\n",
      "iteration #     299, loss = 0.301219\n",
      "iteration #     300, loss = 0.301227\n",
      "iteration #     301, loss = 0.301234\n",
      "iteration #     302, loss = 0.301241\n",
      "iteration #     303, loss = 0.301248\n",
      "iteration #     304, loss = 0.301256\n",
      "iteration #     305, loss = 0.301263\n",
      "iteration #     306, loss = 0.301270\n",
      "iteration #     307, loss = 0.301277\n",
      "iteration #     308, loss = 0.301285\n",
      "iteration #     309, loss = 0.301292\n",
      "iteration #     310, loss = 0.301299\n",
      "iteration #     311, loss = 0.301306\n",
      "iteration #     312, loss = 0.301313\n",
      "iteration #     313, loss = 0.301320\n",
      "iteration #     314, loss = 0.301327\n",
      "iteration #     315, loss = 0.301334\n",
      "iteration #     316, loss = 0.301341\n",
      "iteration #     317, loss = 0.301348\n",
      "iteration #     318, loss = 0.301355\n",
      "iteration #     319, loss = 0.301362\n",
      "iteration #     320, loss = 0.301368\n",
      "iteration #     321, loss = 0.301375\n",
      "iteration #     322, loss = 0.301382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     323, loss = 0.301389\n",
      "iteration #     324, loss = 0.301396\n",
      "iteration #     325, loss = 0.301402\n",
      "iteration #     326, loss = 0.301409\n",
      "iteration #     327, loss = 0.301416\n",
      "iteration #     328, loss = 0.301422\n",
      "iteration #     329, loss = 0.301429\n",
      "iteration #     330, loss = 0.301435\n",
      "iteration #     331, loss = 0.301442\n",
      "iteration #     332, loss = 0.301448\n",
      "iteration #     333, loss = 0.301455\n",
      "iteration #     334, loss = 0.301461\n",
      "iteration #     335, loss = 0.301468\n",
      "iteration #     336, loss = 0.301474\n",
      "iteration #     337, loss = 0.301481\n",
      "iteration #     338, loss = 0.301487\n",
      "iteration #     339, loss = 0.301493\n",
      "iteration #     340, loss = 0.301500\n",
      "iteration #     341, loss = 0.301506\n",
      "iteration #     342, loss = 0.301512\n",
      "iteration #     343, loss = 0.301519\n",
      "iteration #     344, loss = 0.301525\n",
      "iteration #     345, loss = 0.301531\n",
      "iteration #     346, loss = 0.301537\n",
      "iteration #     347, loss = 0.301544\n",
      "iteration #     348, loss = 0.301550\n",
      "iteration #     349, loss = 0.301556\n",
      "iteration #     350, loss = 0.301562\n",
      "iteration #     351, loss = 0.301568\n",
      "iteration #     352, loss = 0.301575\n",
      "iteration #     353, loss = 0.301581\n",
      "iteration #     354, loss = 0.301587\n",
      "iteration #     355, loss = 0.301593\n",
      "iteration #     356, loss = 0.301599\n",
      "iteration #     357, loss = 0.301605\n",
      "iteration #     358, loss = 0.301612\n",
      "iteration #     359, loss = 0.301618\n",
      "iteration #     360, loss = 0.301624\n",
      "iteration #     361, loss = 0.301630\n",
      "iteration #     362, loss = 0.301636\n",
      "iteration #     363, loss = 0.301642\n",
      "iteration #     364, loss = 0.301649\n",
      "iteration #     365, loss = 0.301655\n",
      "iteration #     366, loss = 0.301661\n",
      "iteration #     367, loss = 0.301667\n",
      "iteration #     368, loss = 0.301674\n",
      "iteration #     369, loss = 0.301680\n",
      "iteration #     370, loss = 0.301686\n",
      "iteration #     371, loss = 0.301692\n",
      "iteration #     372, loss = 0.301699\n",
      "iteration #     373, loss = 0.301705\n",
      "iteration #     374, loss = 0.301712\n",
      "iteration #     375, loss = 0.301718\n",
      "iteration #     376, loss = 0.301724\n",
      "iteration #     377, loss = 0.301731\n",
      "iteration #     378, loss = 0.301737\n",
      "iteration #     379, loss = 0.301744\n",
      "iteration #     380, loss = 0.301750\n",
      "iteration #     381, loss = 0.301757\n",
      "iteration #     382, loss = 0.301763\n",
      "iteration #     383, loss = 0.301770\n",
      "iteration #     384, loss = 0.301776\n",
      "iteration #     385, loss = 0.301783\n",
      "iteration #     386, loss = 0.301790\n",
      "iteration #     387, loss = 0.301796\n",
      "iteration #     388, loss = 0.301803\n",
      "iteration #     389, loss = 0.301810\n",
      "iteration #     390, loss = 0.301817\n",
      "iteration #     391, loss = 0.301823\n",
      "iteration #     392, loss = 0.301830\n",
      "iteration #     393, loss = 0.301837\n",
      "iteration #     394, loss = 0.301844\n",
      "iteration #     395, loss = 0.301851\n",
      "iteration #     396, loss = 0.301858\n",
      "iteration #     397, loss = 0.301865\n",
      "iteration #     398, loss = 0.301872\n",
      "iteration #     399, loss = 0.301879\n",
      "iteration #     400, loss = 0.301886\n",
      "iteration #     401, loss = 0.301893\n",
      "iteration #     402, loss = 0.301900\n",
      "iteration #     403, loss = 0.301907\n",
      "iteration #     404, loss = 0.301914\n",
      "iteration #     405, loss = 0.301921\n",
      "iteration #     406, loss = 0.301928\n",
      "iteration #     407, loss = 0.301935\n",
      "iteration #     408, loss = 0.301942\n",
      "iteration #     409, loss = 0.301950\n",
      "iteration #     410, loss = 0.301957\n",
      "iteration #     411, loss = 0.301964\n",
      "iteration #     412, loss = 0.301971\n",
      "iteration #     413, loss = 0.301979\n",
      "iteration #     414, loss = 0.301986\n",
      "iteration #     415, loss = 0.301993\n",
      "iteration #     416, loss = 0.302000\n",
      "iteration #     417, loss = 0.302008\n",
      "iteration #     418, loss = 0.302015\n",
      "iteration #     419, loss = 0.302022\n",
      "iteration #     420, loss = 0.302030\n",
      "iteration #     421, loss = 0.302037\n",
      "iteration #     422, loss = 0.302044\n",
      "iteration #     423, loss = 0.302052\n",
      "iteration #     424, loss = 0.302059\n",
      "iteration #     425, loss = 0.302066\n",
      "iteration #     426, loss = 0.302074\n",
      "iteration #     427, loss = 0.302081\n",
      "iteration #     428, loss = 0.302088\n",
      "iteration #     429, loss = 0.302096\n",
      "iteration #     430, loss = 0.302103\n",
      "iteration #     431, loss = 0.302111\n",
      "iteration #     432, loss = 0.302118\n",
      "iteration #     433, loss = 0.302125\n",
      "iteration #     434, loss = 0.302133\n",
      "iteration #     435, loss = 0.302140\n",
      "iteration #     436, loss = 0.302147\n",
      "iteration #     437, loss = 0.302155\n",
      "iteration #     438, loss = 0.302162\n",
      "iteration #     439, loss = 0.302169\n",
      "iteration #     440, loss = 0.302177\n",
      "iteration #     441, loss = 0.302184\n",
      "iteration #     442, loss = 0.302191\n",
      "iteration #     443, loss = 0.302199\n",
      "iteration #     444, loss = 0.302206\n",
      "iteration #     445, loss = 0.302213\n",
      "iteration #     446, loss = 0.302221\n",
      "iteration #     447, loss = 0.302228\n",
      "iteration #     448, loss = 0.302235\n",
      "iteration #     449, loss = 0.302243\n",
      "iteration #     450, loss = 0.302250\n",
      "iteration #     451, loss = 0.302257\n",
      "iteration #     452, loss = 0.302264\n",
      "iteration #     453, loss = 0.302272\n",
      "iteration #     454, loss = 0.302279\n",
      "iteration #     455, loss = 0.302286\n",
      "iteration #     456, loss = 0.302293\n",
      "iteration #     457, loss = 0.302300\n",
      "iteration #     458, loss = 0.302308\n",
      "iteration #     459, loss = 0.302315\n",
      "iteration #     460, loss = 0.302322\n",
      "iteration #     461, loss = 0.302329\n",
      "iteration #     462, loss = 0.302336\n",
      "iteration #     463, loss = 0.302343\n",
      "iteration #     464, loss = 0.302350\n",
      "iteration #     465, loss = 0.302357\n",
      "iteration #     466, loss = 0.302364\n",
      "iteration #     467, loss = 0.302371\n",
      "iteration #     468, loss = 0.302378\n",
      "iteration #     469, loss = 0.302385\n",
      "iteration #     470, loss = 0.302392\n",
      "iteration #     471, loss = 0.302399\n",
      "iteration #     472, loss = 0.302406\n",
      "iteration #     473, loss = 0.302413\n",
      "iteration #     474, loss = 0.302420\n",
      "iteration #     475, loss = 0.302427\n",
      "iteration #     476, loss = 0.302434\n",
      "iteration #     477, loss = 0.302441\n",
      "iteration #     478, loss = 0.302448\n",
      "iteration #     479, loss = 0.302454\n",
      "iteration #     480, loss = 0.302461\n",
      "iteration #     481, loss = 0.302468\n",
      "iteration #     482, loss = 0.302475\n",
      "iteration #     483, loss = 0.302481\n",
      "iteration #     484, loss = 0.302488\n",
      "iteration #     485, loss = 0.302495\n",
      "iteration #     486, loss = 0.302502\n",
      "iteration #     487, loss = 0.302508\n",
      "iteration #     488, loss = 0.302515\n",
      "iteration #     489, loss = 0.302521\n",
      "iteration #     490, loss = 0.302528\n",
      "iteration #     491, loss = 0.302535\n",
      "iteration #     492, loss = 0.302541\n",
      "iteration #     493, loss = 0.302548\n",
      "iteration #     494, loss = 0.302554\n",
      "iteration #     495, loss = 0.302561\n",
      "iteration #     496, loss = 0.302567\n",
      "iteration #     497, loss = 0.302574\n",
      "iteration #     498, loss = 0.302580\n",
      "iteration #     499, loss = 0.302587\n",
      "iteration #     500, loss = 0.302593\n",
      "iteration #     501, loss = 0.302599\n",
      "iteration #     502, loss = 0.302606\n",
      "iteration #     503, loss = 0.302612\n",
      "iteration #     504, loss = 0.302618\n",
      "iteration #     505, loss = 0.302625\n",
      "iteration #     506, loss = 0.302631\n",
      "iteration #     507, loss = 0.302637\n",
      "iteration #     508, loss = 0.302643\n",
      "iteration #     509, loss = 0.302649\n",
      "iteration #     510, loss = 0.302656\n",
      "iteration #     511, loss = 0.302662\n",
      "iteration #     512, loss = 0.302668\n",
      "iteration #     513, loss = 0.302674\n",
      "iteration #     514, loss = 0.302680\n",
      "iteration #     515, loss = 0.302686\n",
      "iteration #     516, loss = 0.302692\n",
      "iteration #     517, loss = 0.302698\n",
      "iteration #     518, loss = 0.302704\n",
      "iteration #     519, loss = 0.302710\n",
      "iteration #     520, loss = 0.302716\n",
      "iteration #     521, loss = 0.302722\n",
      "iteration #     522, loss = 0.302728\n",
      "iteration #     523, loss = 0.302734\n",
      "iteration #     524, loss = 0.302740\n",
      "iteration #     525, loss = 0.302746\n",
      "iteration #     526, loss = 0.302752\n",
      "iteration #     527, loss = 0.302758\n",
      "iteration #     528, loss = 0.302763\n",
      "iteration #     529, loss = 0.302769\n",
      "iteration #     530, loss = 0.302775\n",
      "iteration #     531, loss = 0.302781\n",
      "iteration #     532, loss = 0.302786\n",
      "iteration #     533, loss = 0.302792\n",
      "iteration #     534, loss = 0.302798\n",
      "iteration #     535, loss = 0.302803\n",
      "iteration #     536, loss = 0.302809\n",
      "iteration #     537, loss = 0.302815\n",
      "iteration #     538, loss = 0.302820\n",
      "iteration #     539, loss = 0.302826\n",
      "iteration #     540, loss = 0.302831\n",
      "iteration #     541, loss = 0.302837\n",
      "iteration #     542, loss = 0.302842\n",
      "iteration #     543, loss = 0.302848\n",
      "iteration #     544, loss = 0.302853\n",
      "iteration #     545, loss = 0.302859\n",
      "iteration #     546, loss = 0.302864\n",
      "iteration #     547, loss = 0.302870\n",
      "iteration #     548, loss = 0.302875\n",
      "iteration #     549, loss = 0.302881\n",
      "iteration #     550, loss = 0.302886\n",
      "iteration #     551, loss = 0.302891\n",
      "iteration #     552, loss = 0.302897\n",
      "iteration #     553, loss = 0.302902\n",
      "iteration #     554, loss = 0.302907\n",
      "iteration #     555, loss = 0.302912\n",
      "iteration #     556, loss = 0.302918\n",
      "iteration #     557, loss = 0.302923\n",
      "iteration #     558, loss = 0.302928\n",
      "iteration #     559, loss = 0.302933\n",
      "iteration #     560, loss = 0.302938\n",
      "iteration #     561, loss = 0.302944\n",
      "iteration #     562, loss = 0.302949\n",
      "iteration #     563, loss = 0.302954\n",
      "iteration #     564, loss = 0.302959\n",
      "iteration #     565, loss = 0.302964\n",
      "iteration #     566, loss = 0.302969\n",
      "iteration #     567, loss = 0.302974\n",
      "iteration #     568, loss = 0.302979\n",
      "iteration #     569, loss = 0.302984\n",
      "iteration #     570, loss = 0.302989\n",
      "iteration #     571, loss = 0.302994\n",
      "iteration #     572, loss = 0.302999\n",
      "iteration #     573, loss = 0.303004\n",
      "iteration #     574, loss = 0.303009\n",
      "iteration #     575, loss = 0.303014\n",
      "iteration #     576, loss = 0.303019\n",
      "iteration #     577, loss = 0.303023\n",
      "iteration #     578, loss = 0.303028\n",
      "iteration #     579, loss = 0.303033\n",
      "iteration #     580, loss = 0.303038\n",
      "iteration #     581, loss = 0.303043\n",
      "iteration #     582, loss = 0.303048\n",
      "iteration #     583, loss = 0.303052\n",
      "iteration #     584, loss = 0.303057\n",
      "iteration #     585, loss = 0.303062\n",
      "iteration #     586, loss = 0.303066\n",
      "iteration #     587, loss = 0.303071\n",
      "iteration #     588, loss = 0.303076\n",
      "iteration #     589, loss = 0.303080\n",
      "iteration #     590, loss = 0.303085\n",
      "iteration #     591, loss = 0.303090\n",
      "iteration #     592, loss = 0.303094\n",
      "iteration #     593, loss = 0.303099\n",
      "iteration #     594, loss = 0.303103\n",
      "iteration #     595, loss = 0.303108\n",
      "iteration #     596, loss = 0.303113\n",
      "iteration #     597, loss = 0.303117\n",
      "iteration #     598, loss = 0.303122\n",
      "iteration #     599, loss = 0.303126\n",
      "iteration #     600, loss = 0.303131\n",
      "iteration #     601, loss = 0.303135\n",
      "iteration #     602, loss = 0.303139\n",
      "iteration #     603, loss = 0.303144\n",
      "iteration #     604, loss = 0.303148\n",
      "iteration #     605, loss = 0.303153\n",
      "iteration #     606, loss = 0.303157\n",
      "iteration #     607, loss = 0.303161\n",
      "iteration #     608, loss = 0.303166\n",
      "iteration #     609, loss = 0.303170\n",
      "iteration #     610, loss = 0.303174\n",
      "iteration #     611, loss = 0.303179\n",
      "iteration #     612, loss = 0.303183\n",
      "iteration #     613, loss = 0.303187\n",
      "iteration #     614, loss = 0.303192\n",
      "iteration #     615, loss = 0.303196\n",
      "iteration #     616, loss = 0.303200\n",
      "iteration #     617, loss = 0.303204\n",
      "iteration #     618, loss = 0.303208\n",
      "iteration #     619, loss = 0.303213\n",
      "iteration #     620, loss = 0.303217\n",
      "iteration #     621, loss = 0.303221\n",
      "iteration #     622, loss = 0.303225\n",
      "iteration #     623, loss = 0.303229\n",
      "iteration #     624, loss = 0.303233\n",
      "iteration #     625, loss = 0.303238\n",
      "iteration #     626, loss = 0.303242\n",
      "iteration #     627, loss = 0.303246\n",
      "iteration #     628, loss = 0.303250\n",
      "iteration #     629, loss = 0.303254\n",
      "iteration #     630, loss = 0.303258\n",
      "iteration #     631, loss = 0.303262\n",
      "iteration #     632, loss = 0.303266\n",
      "iteration #     633, loss = 0.303270\n",
      "iteration #     634, loss = 0.303274\n",
      "iteration #     635, loss = 0.303278\n",
      "iteration #     636, loss = 0.303282\n",
      "iteration #     637, loss = 0.303286\n",
      "iteration #     638, loss = 0.303290\n",
      "iteration #     639, loss = 0.303294\n",
      "iteration #     640, loss = 0.303298\n",
      "iteration #     641, loss = 0.303302\n",
      "iteration #     642, loss = 0.303306\n",
      "iteration #     643, loss = 0.303309\n",
      "iteration #     644, loss = 0.303313\n",
      "iteration #     645, loss = 0.303317\n",
      "iteration #     646, loss = 0.303321\n",
      "iteration #     647, loss = 0.303325\n",
      "iteration #     648, loss = 0.303329\n",
      "iteration #     649, loss = 0.303332\n",
      "iteration #     650, loss = 0.303336\n",
      "iteration #     651, loss = 0.303340\n",
      "iteration #     652, loss = 0.303344\n",
      "iteration #     653, loss = 0.303348\n",
      "iteration #     654, loss = 0.303351\n",
      "iteration #     655, loss = 0.303355\n",
      "iteration #     656, loss = 0.303359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     657, loss = 0.303363\n",
      "iteration #     658, loss = 0.303366\n",
      "iteration #     659, loss = 0.303370\n",
      "iteration #     660, loss = 0.303374\n",
      "iteration #     661, loss = 0.303377\n",
      "iteration #     662, loss = 0.303381\n",
      "iteration #     663, loss = 0.303385\n",
      "iteration #     664, loss = 0.303388\n",
      "iteration #     665, loss = 0.303392\n",
      "iteration #     666, loss = 0.303396\n",
      "iteration #     667, loss = 0.303399\n",
      "iteration #     668, loss = 0.303403\n",
      "iteration #     669, loss = 0.303406\n",
      "iteration #     670, loss = 0.303410\n",
      "iteration #     671, loss = 0.303414\n",
      "iteration #     672, loss = 0.303417\n",
      "iteration #     673, loss = 0.303421\n",
      "iteration #     674, loss = 0.303424\n",
      "iteration #     675, loss = 0.303428\n",
      "iteration #     676, loss = 0.303431\n",
      "iteration #     677, loss = 0.303435\n",
      "iteration #     678, loss = 0.303438\n",
      "iteration #     679, loss = 0.303442\n",
      "iteration #     680, loss = 0.303445\n",
      "iteration #     681, loss = 0.303449\n",
      "iteration #     682, loss = 0.303452\n",
      "iteration #     683, loss = 0.303456\n",
      "iteration #     684, loss = 0.303459\n",
      "iteration #     685, loss = 0.303463\n",
      "iteration #     686, loss = 0.303466\n",
      "iteration #     687, loss = 0.303469\n",
      "iteration #     688, loss = 0.303473\n",
      "iteration #     689, loss = 0.303476\n",
      "iteration #     690, loss = 0.303480\n",
      "iteration #     691, loss = 0.303483\n",
      "iteration #     692, loss = 0.303486\n",
      "iteration #     693, loss = 0.303490\n",
      "iteration #     694, loss = 0.303493\n",
      "iteration #     695, loss = 0.303496\n",
      "iteration #     696, loss = 0.303500\n",
      "iteration #     697, loss = 0.303503\n",
      "iteration #     698, loss = 0.303506\n",
      "iteration #     699, loss = 0.303510\n",
      "iteration #     700, loss = 0.303513\n",
      "iteration #     701, loss = 0.303516\n",
      "iteration #     702, loss = 0.303519\n",
      "iteration #     703, loss = 0.303523\n",
      "iteration #     704, loss = 0.303526\n",
      "iteration #     705, loss = 0.303529\n",
      "iteration #     706, loss = 0.303532\n",
      "iteration #     707, loss = 0.303536\n",
      "iteration #     708, loss = 0.303539\n",
      "iteration #     709, loss = 0.303542\n",
      "iteration #     710, loss = 0.303545\n",
      "iteration #     711, loss = 0.303549\n",
      "iteration #     712, loss = 0.303552\n",
      "iteration #     713, loss = 0.303555\n",
      "iteration #     714, loss = 0.303558\n",
      "iteration #     715, loss = 0.303561\n",
      "iteration #     716, loss = 0.303564\n",
      "iteration #     717, loss = 0.303568\n",
      "iteration #     718, loss = 0.303571\n",
      "iteration #     719, loss = 0.303574\n",
      "iteration #     720, loss = 0.303577\n",
      "iteration #     721, loss = 0.303580\n",
      "iteration #     722, loss = 0.303583\n",
      "iteration #     723, loss = 0.303586\n",
      "iteration #     724, loss = 0.303589\n",
      "iteration #     725, loss = 0.303592\n",
      "iteration #     726, loss = 0.303595\n",
      "iteration #     727, loss = 0.303599\n",
      "iteration #     728, loss = 0.303602\n",
      "iteration #     729, loss = 0.303605\n",
      "iteration #     730, loss = 0.303608\n",
      "iteration #     731, loss = 0.303611\n",
      "iteration #     732, loss = 0.303614\n",
      "iteration #     733, loss = 0.303617\n",
      "iteration #     734, loss = 0.303620\n",
      "iteration #     735, loss = 0.303623\n",
      "iteration #     736, loss = 0.303626\n",
      "iteration #     737, loss = 0.303629\n",
      "iteration #     738, loss = 0.303632\n",
      "iteration #     739, loss = 0.303635\n",
      "iteration #     740, loss = 0.303638\n",
      "iteration #     741, loss = 0.303641\n",
      "iteration #     742, loss = 0.303643\n",
      "iteration #     743, loss = 0.303646\n",
      "iteration #     744, loss = 0.303649\n",
      "iteration #     745, loss = 0.303652\n",
      "iteration #     746, loss = 0.303655\n",
      "iteration #     747, loss = 0.303658\n",
      "iteration #     748, loss = 0.303661\n",
      "iteration #     749, loss = 0.303664\n",
      "iteration #     750, loss = 0.303667\n",
      "iteration #     751, loss = 0.303670\n",
      "iteration #     752, loss = 0.303672\n",
      "iteration #     753, loss = 0.303675\n",
      "iteration #     754, loss = 0.303678\n",
      "iteration #     755, loss = 0.303681\n",
      "iteration #     756, loss = 0.303684\n",
      "iteration #     757, loss = 0.303687\n",
      "iteration #     758, loss = 0.303690\n",
      "iteration #     759, loss = 0.303692\n",
      "iteration #     760, loss = 0.303695\n",
      "iteration #     761, loss = 0.303698\n",
      "iteration #     762, loss = 0.303701\n",
      "iteration #     763, loss = 0.303704\n",
      "iteration #     764, loss = 0.303706\n",
      "iteration #     765, loss = 0.303709\n",
      "iteration #     766, loss = 0.303712\n",
      "iteration #     767, loss = 0.303715\n",
      "iteration #     768, loss = 0.303717\n",
      "iteration #     769, loss = 0.303720\n",
      "iteration #     770, loss = 0.303723\n",
      "iteration #     771, loss = 0.303726\n",
      "iteration #     772, loss = 0.303728\n",
      "iteration #     773, loss = 0.303731\n",
      "iteration #     774, loss = 0.303734\n",
      "iteration #     775, loss = 0.303736\n",
      "iteration #     776, loss = 0.303739\n",
      "iteration #     777, loss = 0.303742\n",
      "iteration #     778, loss = 0.303745\n",
      "iteration #     779, loss = 0.303747\n",
      "iteration #     780, loss = 0.303750\n",
      "iteration #     781, loss = 0.303753\n",
      "iteration #     782, loss = 0.303755\n",
      "iteration #     783, loss = 0.303758\n",
      "iteration #     784, loss = 0.303761\n",
      "iteration #     785, loss = 0.303763\n",
      "iteration #     786, loss = 0.303766\n",
      "iteration #     787, loss = 0.303768\n",
      "iteration #     788, loss = 0.303771\n",
      "iteration #     789, loss = 0.303774\n",
      "iteration #     790, loss = 0.303776\n",
      "iteration #     791, loss = 0.303779\n",
      "iteration #     792, loss = 0.303781\n",
      "iteration #     793, loss = 0.303784\n",
      "iteration #     794, loss = 0.303787\n",
      "iteration #     795, loss = 0.303789\n",
      "iteration #     796, loss = 0.303792\n",
      "iteration #     797, loss = 0.303794\n",
      "iteration #     798, loss = 0.303797\n",
      "iteration #     799, loss = 0.303799\n",
      "iteration #     800, loss = 0.303802\n",
      "iteration #     801, loss = 0.303804\n",
      "iteration #     802, loss = 0.303807\n",
      "iteration #     803, loss = 0.303810\n",
      "iteration #     804, loss = 0.303812\n",
      "iteration #     805, loss = 0.303815\n",
      "iteration #     806, loss = 0.303817\n",
      "iteration #     807, loss = 0.303820\n",
      "iteration #     808, loss = 0.303822\n",
      "iteration #     809, loss = 0.303825\n",
      "iteration #     810, loss = 0.303827\n",
      "iteration #     811, loss = 0.303829\n",
      "iteration #     812, loss = 0.303832\n",
      "iteration #     813, loss = 0.303834\n",
      "iteration #     814, loss = 0.303837\n",
      "iteration #     815, loss = 0.303839\n",
      "iteration #     816, loss = 0.303842\n",
      "iteration #     817, loss = 0.303844\n",
      "iteration #     818, loss = 0.303847\n",
      "iteration #     819, loss = 0.303849\n",
      "iteration #     820, loss = 0.303851\n",
      "iteration #     821, loss = 0.303854\n",
      "iteration #     822, loss = 0.303856\n",
      "iteration #     823, loss = 0.303859\n",
      "iteration #     824, loss = 0.303861\n",
      "iteration #     825, loss = 0.303863\n",
      "iteration #     826, loss = 0.303866\n",
      "iteration #     827, loss = 0.303868\n",
      "iteration #     828, loss = 0.303871\n",
      "iteration #     829, loss = 0.303873\n",
      "iteration #     830, loss = 0.303875\n",
      "iteration #     831, loss = 0.303878\n",
      "iteration #     832, loss = 0.303880\n",
      "iteration #     833, loss = 0.303882\n",
      "iteration #     834, loss = 0.303885\n",
      "iteration #     835, loss = 0.303887\n",
      "iteration #     836, loss = 0.303889\n",
      "iteration #     837, loss = 0.303892\n",
      "iteration #     838, loss = 0.303894\n",
      "iteration #     839, loss = 0.303896\n",
      "iteration #     840, loss = 0.303899\n",
      "iteration #     841, loss = 0.303901\n",
      "iteration #     842, loss = 0.303903\n",
      "iteration #     843, loss = 0.303905\n",
      "iteration #     844, loss = 0.303908\n",
      "iteration #     845, loss = 0.303910\n",
      "iteration #     846, loss = 0.303912\n",
      "iteration #     847, loss = 0.303915\n",
      "iteration #     848, loss = 0.303917\n",
      "iteration #     849, loss = 0.303919\n",
      "iteration #     850, loss = 0.303921\n",
      "iteration #     851, loss = 0.303923\n",
      "iteration #     852, loss = 0.303926\n",
      "iteration #     853, loss = 0.303928\n",
      "iteration #     854, loss = 0.303930\n",
      "iteration #     855, loss = 0.303932\n",
      "iteration #     856, loss = 0.303935\n",
      "iteration #     857, loss = 0.303937\n",
      "iteration #     858, loss = 0.303939\n",
      "iteration #     859, loss = 0.303941\n",
      "iteration #     860, loss = 0.303943\n",
      "iteration #     861, loss = 0.303946\n",
      "iteration #     862, loss = 0.303948\n",
      "iteration #     863, loss = 0.303950\n",
      "iteration #     864, loss = 0.303952\n",
      "iteration #     865, loss = 0.303954\n",
      "iteration #     866, loss = 0.303956\n",
      "iteration #     867, loss = 0.303959\n",
      "iteration #     868, loss = 0.303961\n",
      "iteration #     869, loss = 0.303963\n",
      "iteration #     870, loss = 0.303965\n",
      "iteration #     871, loss = 0.303967\n",
      "iteration #     872, loss = 0.303969\n",
      "iteration #     873, loss = 0.303971\n",
      "iteration #     874, loss = 0.303974\n",
      "iteration #     875, loss = 0.303976\n",
      "iteration #     876, loss = 0.303978\n",
      "iteration #     877, loss = 0.303980\n",
      "iteration #     878, loss = 0.303982\n",
      "iteration #     879, loss = 0.303984\n",
      "iteration #     880, loss = 0.303986\n",
      "iteration #     881, loss = 0.303988\n",
      "iteration #     882, loss = 0.303990\n",
      "iteration #     883, loss = 0.303992\n",
      "iteration #     884, loss = 0.303994\n",
      "iteration #     885, loss = 0.303996\n",
      "iteration #     886, loss = 0.303999\n",
      "iteration #     887, loss = 0.304001\n",
      "iteration #     888, loss = 0.304003\n",
      "iteration #     889, loss = 0.304005\n",
      "iteration #     890, loss = 0.304007\n",
      "iteration #     891, loss = 0.304009\n",
      "iteration #     892, loss = 0.304011\n",
      "iteration #     893, loss = 0.304013\n",
      "iteration #     894, loss = 0.304015\n",
      "iteration #     895, loss = 0.304017\n",
      "iteration #     896, loss = 0.304019\n",
      "iteration #     897, loss = 0.304021\n",
      "iteration #     898, loss = 0.304023\n",
      "iteration #     899, loss = 0.304025\n",
      "iteration #     900, loss = 0.304027\n",
      "iteration #     901, loss = 0.304029\n",
      "iteration #     902, loss = 0.304031\n",
      "iteration #     903, loss = 0.304033\n",
      "iteration #     904, loss = 0.304035\n",
      "iteration #     905, loss = 0.304037\n",
      "iteration #     906, loss = 0.304039\n",
      "iteration #     907, loss = 0.304041\n",
      "iteration #     908, loss = 0.304042\n",
      "iteration #     909, loss = 0.304044\n",
      "iteration #     910, loss = 0.304046\n",
      "iteration #     911, loss = 0.304048\n",
      "iteration #     912, loss = 0.304050\n",
      "iteration #     913, loss = 0.304052\n",
      "iteration #     914, loss = 0.304054\n",
      "iteration #     915, loss = 0.304056\n",
      "iteration #     916, loss = 0.304058\n",
      "iteration #     917, loss = 0.304060\n",
      "iteration #     918, loss = 0.304062\n",
      "iteration #     919, loss = 0.304064\n",
      "iteration #     920, loss = 0.304065\n",
      "iteration #     921, loss = 0.304067\n",
      "iteration #     922, loss = 0.304069\n",
      "iteration #     923, loss = 0.304071\n",
      "iteration #     924, loss = 0.304073\n",
      "iteration #     925, loss = 0.304075\n",
      "iteration #     926, loss = 0.304077\n",
      "iteration #     927, loss = 0.304079\n",
      "iteration #     928, loss = 0.304080\n",
      "iteration #     929, loss = 0.304082\n",
      "iteration #     930, loss = 0.304084\n",
      "iteration #     931, loss = 0.304086\n",
      "iteration #     932, loss = 0.304088\n",
      "iteration #     933, loss = 0.304090\n",
      "iteration #     934, loss = 0.304091\n",
      "iteration #     935, loss = 0.304093\n",
      "iteration #     936, loss = 0.304095\n",
      "iteration #     937, loss = 0.304097\n",
      "iteration #     938, loss = 0.304099\n",
      "iteration #     939, loss = 0.304100\n",
      "iteration #     940, loss = 0.304102\n",
      "iteration #     941, loss = 0.304104\n",
      "iteration #     942, loss = 0.304106\n",
      "iteration #     943, loss = 0.304108\n",
      "iteration #     944, loss = 0.304109\n",
      "iteration #     945, loss = 0.304111\n",
      "iteration #     946, loss = 0.304113\n",
      "iteration #     947, loss = 0.304115\n",
      "iteration #     948, loss = 0.304116\n",
      "iteration #     949, loss = 0.304118\n",
      "iteration #     950, loss = 0.304120\n",
      "iteration #     951, loss = 0.304122\n",
      "iteration #     952, loss = 0.304123\n",
      "iteration #     953, loss = 0.304125\n",
      "iteration #     954, loss = 0.304127\n",
      "iteration #     955, loss = 0.304129\n",
      "iteration #     956, loss = 0.304130\n",
      "iteration #     957, loss = 0.304132\n",
      "iteration #     958, loss = 0.304134\n",
      "iteration #     959, loss = 0.304135\n",
      "iteration #     960, loss = 0.304137\n",
      "iteration #     961, loss = 0.304139\n",
      "iteration #     962, loss = 0.304140\n",
      "iteration #     963, loss = 0.304142\n",
      "iteration #     964, loss = 0.304144\n",
      "iteration #     965, loss = 0.304145\n",
      "iteration #     966, loss = 0.304147\n",
      "iteration #     967, loss = 0.304149\n",
      "iteration #     968, loss = 0.304151\n",
      "iteration #     969, loss = 0.304152\n",
      "iteration #     970, loss = 0.304154\n",
      "iteration #     971, loss = 0.304155\n",
      "iteration #     972, loss = 0.304157\n",
      "iteration #     973, loss = 0.304159\n",
      "iteration #     974, loss = 0.304160\n",
      "iteration #     975, loss = 0.304162\n",
      "iteration #     976, loss = 0.304164\n",
      "iteration #     977, loss = 0.304165\n",
      "iteration #     978, loss = 0.304167\n",
      "iteration #     979, loss = 0.304169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     980, loss = 0.304170\n",
      "iteration #     981, loss = 0.304172\n",
      "iteration #     982, loss = 0.304173\n",
      "iteration #     983, loss = 0.304175\n",
      "iteration #     984, loss = 0.304177\n",
      "iteration #     985, loss = 0.304178\n",
      "iteration #     986, loss = 0.304180\n",
      "iteration #     987, loss = 0.304181\n",
      "iteration #     988, loss = 0.304183\n",
      "iteration #     989, loss = 0.304184\n",
      "iteration #     990, loss = 0.304186\n",
      "iteration #     991, loss = 0.304188\n",
      "iteration #     992, loss = 0.304189\n",
      "iteration #     993, loss = 0.304191\n",
      "iteration #     994, loss = 0.304192\n",
      "iteration #     995, loss = 0.304194\n",
      "iteration #     996, loss = 0.304195\n",
      "iteration #     997, loss = 0.304197\n",
      "iteration #     998, loss = 0.304198\n",
      "iteration #     999, loss = 0.304200\n",
      "iteration #    1000, loss = 0.304202\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(1, x_training_aug.shape[1])\n",
    "theta = bgd(x_training_aug, y_training, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the parameters are learned, we can test the performance of the classifier. Note that logistic regression returns a number in [0,1], thus we need to binarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression test error = 0.9914\n"
     ]
    }
   ],
   "source": [
    "y_prediction = (h(x_testing_aug, theta)>=0.5)[:,0]\n",
    "test_error = np.sum(y_testing != y_prediction) / y_testing.shape[0]\n",
    "print('logistic regression test error = {:.4f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
