{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Websites\n",
    "\n",
    "\n",
    "## About the Dataset\n",
    "This dataset consists of 11055 training data set and 2456 testing data set. It has 30 attributes, namely, \n",
    "- having_IP_Address  { -1,1 }\n",
    "- URL_Length   { 1,0,-1 } \n",
    "- Shortining_Service { 1,-1 } \n",
    "- having_At_Symbol   { 1,-1 } \n",
    "- double_slash_redirecting { -1,1 } \n",
    "- Prefix_Suffix  { -1,1 } \n",
    "- having_Sub_Domain  { -1,0,1 } \n",
    "- SSLfinal_State  { -1,1,0 } \n",
    "- Domain_registeration_length { -1,1 } \n",
    "- Favicon { 1,-1 } \n",
    "- port { 1,-1 } \n",
    "- HTTPS_token { -1,1 } \n",
    "- Request_URL  { 1,-1 } \n",
    "- URL_of_Anchor { -1,0,1 } \n",
    "- Links_in_tags { 1,-1,0 } \n",
    "- SFH  { -1,1,0 } \n",
    "- Submitting_to_email { -1,1 } \n",
    "- Abnormal_URL { -1,1 }\n",
    "- Redirect  { 0,1 } \n",
    "- on_mouseover  { 1,-1 }\n",
    "- RightClick  { 1,-1 } \n",
    "- popUpWidnow  { 1,-1 } \n",
    "- Iframe { 1,-1 } \n",
    "- age_of_domain  { -1,1 } \n",
    "- DNSRecord   { -1,1 } \n",
    "- web_traffic  { -1,0,1 } \n",
    "- Page_Rank { -1,1 } \n",
    "- Google_Index { 1,-1 } \n",
    "- Links_pointing_to_page { 1,0,-1 } \n",
    "- Statistical_report { -1,1 } \n",
    "- Result  { -1,1 } \n",
    "\n",
    "which they take on the values 1, 0, -1 that mean whether the website is either legitimate, suspicious or phishing, respectively.\n",
    "\n",
    "###### The following segment of code just creates and duplicates the dataset in the .arff files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Getting all the arff files from the current directory\n",
    "files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
    "\n",
    "# Function for converting arff list to csv list\n",
    "def toCsv(content):\n",
    "    data = False\n",
    "    header = \"\"\n",
    "    newContent = []\n",
    "    for line in content:\n",
    "        if not data:\n",
    "            if \"@attribute\" in line:\n",
    "                attri = line.split()\n",
    "                columnName = attri[attri.index(\"@attribute\")+1]\n",
    "                header = header + columnName + \",\"\n",
    "            elif \"@data\" in line:\n",
    "                data = True\n",
    "                header = header[:-1]\n",
    "                header += '\\n'\n",
    "                newContent.append(header)\n",
    "        else:\n",
    "            newContent.append(line)\n",
    "    return newContent\n",
    "\n",
    "# Main loop for reading and writing files\n",
    "for file in files:\n",
    "    with open(file , \"r\") as inFile:\n",
    "        content = inFile.readlines()\n",
    "        name,ext = os.path.splitext(inFile.name)\n",
    "        new = toCsv(content)\n",
    "        with open(name+\".csv\", \"w\") as outFile:\n",
    "            outFile.writelines(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset : \n",
      "number of samples_______ = 11055\n",
      "number of negative ones_ = 4898\n",
      "number of ones__________ = 6157\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing dataset : \n",
      "number of samples_______ = 2456\n",
      "number of negative ones_ = 1362\n",
      "number of ones__________ = 1094\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"white\")\n",
    "sns.set(style = \"whitegrid\", color_codes = True)\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('Training Dataset.csv', sep = ',')\n",
    "data_test = pd.read_csv('old.csv', sep = ',')\n",
    "\n",
    "feature_index_names = {0:'having_IP_Address', 1:'URL_Length', 2:'Shortining_Service', 3:'having_At_Symbol', \n",
    "                        4:'double_slash_redirecting', 5:'Prefix_Suffix', 6:'having_Sub_Domain', 7:'SSLfinal_State',\n",
    "                        8:'Domain_registeration_length', 9:'Favicon', 10:'port', 11:'HTTPS_token',12:'Request_URL', \n",
    "                        13:'URL_of_Anchor', 14:'Links_in_tags', 15:'SFH', 16:'Submitting_to_email', 17:'Abnormal_URL', \n",
    "                        18:'Redirect', 19:'on_mouseover', 20:'RightClick', 21:'popUpWidnow', 22:'Iframe',\n",
    "                        23:' age_of_domain', 24:'DNSRecord', 25:'web_traffic', 26:'Page_Rank', 27:' Google_Index', \n",
    "                        28:'Links_pointing_to_page', 29:'Statistical_report'} \n",
    "#training dataset\n",
    "training_data = np.array(data_train)\n",
    "x_training = training_data[:, :-1]\n",
    "y_training = training_data[:, -1]\n",
    "\n",
    "print('training dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_training.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_training == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_training == 1)))\n",
    "\n",
    "sns.countplot(x = 'Result', data = data_train, palette = 'hls')\n",
    "plt.title('Training dataset')\n",
    "plt.show()\n",
    "\n",
    "#testing dataset\n",
    "testing_data = np.array(data_test)\n",
    "x_testing = testing_data[:, :-1]\n",
    "y_testing = testing_data[:, -1]\n",
    "\n",
    "print('testing dataset : ')\n",
    "print('{:_<24s} = {:d}'.format('number of samples', y_testing.shape[0]))\n",
    "print('{:_<24s} = {:d}'.format('number of negative ones', np.sum(y_testing == -1)))\n",
    "print('{:_<24s} = {:d}'.format('number of ones', np.sum(y_testing == 1)))\n",
    "sns.countplot(x = 'Result', data = data_test, palette = 'hls')\n",
    "plt.title('Testing dataset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "We can also use logistic regression to perform occupancy detection. In order to achieve this, first we need to define our hypothesis (or model):\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "h_{\\boldsymbol{\\Theta}}(\\mathbf{x}) & = & \\frac{1}{1 + e^{-(\\theta_{0} x_{0} + \\theta_{1} x_{1} + \\cdots + \\theta_{d} x_{d})}} \\\\\n",
    "& = & \\frac{1}{1 + e^{-\\mathbf{x} \\boldsymbol{\\Theta}^{T}}},\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\mathbf{x} = [x_{0}, x_{1}, \\ldots, x_{d}]$, $\\boldsymbol{\\Theta}=[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{d}]$ and $x_{0} = 1$. \n",
    "\n",
    "The following function implements this model, which assumes that all vectors are row-major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, theta):\n",
    "    s = np.dot(x, theta.T)\n",
    "    u = 1.0 / (1.0 + np.exp(-s))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define our loss function to learn the parameters of the model based on training dataset. We can use mean square error as our loss function, i.e.,\n",
    "\n",
    "$$\n",
    "J\\left(\\boldsymbol{\\Theta}\\right) = \\frac{1}{2 N} \\sum\\limits_{n=1}^{N} \\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) - y^{\\left(n\\right)} \\right)^2 = \\frac{1}{N} \\sum\\limits_{n=1}^{N}Cost\\left(h_{ \\boldsymbol{\\Theta}}\\left(\\mathbf{x}^{\\left(n\\right)}\\right) , y^{\\left(n\\right)}\\right).\n",
    "$$\n",
    "\n",
    "The following function implements the loss function $J\\left(\\boldsymbol{\\Theta}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, theta):\n",
    "    N = y.shape[0]\n",
    "    mse = 1.0 / (2*N) * np.sum((h(x, theta) - y)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need implement gradient descent solver to learn the parameters $\\boldsymbol{\\Theta}$ of our model. The following function implements batch gradient descent to learn the parameters $\\boldsymbol{\\Theta}$ on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(x, y, theta, alpha=0.00155, epsilon=0.001, max_iter=1000):\n",
    "    y = y[:, np.newaxis]\n",
    "    N = y.shape[0]\n",
    "    t = 0\n",
    "    while True:\n",
    "        # print the value of loss function for each iteration\n",
    "        print('iteration #{:>8d}, loss = {:>8f}'.format(t, J(x,y,theta)))\n",
    "        # keep a copy of the parameter vector before the update for checking the convergence criterion\n",
    "        theta_previous = theta.copy()\n",
    "        # update the parameter vector\n",
    "        e = (h(x, theta) - y)\n",
    "        theta  = theta - alpha * 1.0 / N * np.sum( e * x, axis=0)\n",
    "        t = t + 1\n",
    "        # check the convergence criterion\n",
    "        if (np.max(np.abs(theta-theta_previous)) < epsilon) or (t>max_iter):\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(x_training, axis=0)\n",
    "s = np.std(x_training, axis=0)\n",
    "x_training = (x_training - m) / s\n",
    "x_testing = (x_testing - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment training and testing vector to take advantage of fast matrix operations in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_aug = np.hstack((np.ones((x_training.shape[0],1)), x_training))\n",
    "x_testing_aug = np.hstack((np.ones((x_testing.shape[0],1)), x_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random parameter vector and learn the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #       0, loss = 0.727020\n",
      "iteration #       1, loss = 0.726745\n",
      "iteration #       2, loss = 0.726469\n",
      "iteration #       3, loss = 0.726194\n",
      "iteration #       4, loss = 0.725919\n",
      "iteration #       5, loss = 0.725643\n",
      "iteration #       6, loss = 0.725367\n",
      "iteration #       7, loss = 0.725092\n",
      "iteration #       8, loss = 0.724816\n",
      "iteration #       9, loss = 0.724540\n",
      "iteration #      10, loss = 0.724265\n",
      "iteration #      11, loss = 0.723989\n",
      "iteration #      12, loss = 0.723713\n",
      "iteration #      13, loss = 0.723437\n",
      "iteration #      14, loss = 0.723161\n",
      "iteration #      15, loss = 0.722885\n",
      "iteration #      16, loss = 0.722609\n",
      "iteration #      17, loss = 0.722333\n",
      "iteration #      18, loss = 0.722057\n",
      "iteration #      19, loss = 0.721781\n",
      "iteration #      20, loss = 0.721504\n",
      "iteration #      21, loss = 0.721228\n",
      "iteration #      22, loss = 0.720952\n",
      "iteration #      23, loss = 0.720675\n",
      "iteration #      24, loss = 0.720399\n",
      "iteration #      25, loss = 0.720122\n",
      "iteration #      26, loss = 0.719846\n",
      "iteration #      27, loss = 0.719569\n",
      "iteration #      28, loss = 0.719292\n",
      "iteration #      29, loss = 0.719016\n",
      "iteration #      30, loss = 0.718739\n",
      "iteration #      31, loss = 0.718462\n",
      "iteration #      32, loss = 0.718185\n",
      "iteration #      33, loss = 0.717909\n",
      "iteration #      34, loss = 0.717632\n",
      "iteration #      35, loss = 0.717355\n",
      "iteration #      36, loss = 0.717078\n",
      "iteration #      37, loss = 0.716801\n",
      "iteration #      38, loss = 0.716524\n",
      "iteration #      39, loss = 0.716247\n",
      "iteration #      40, loss = 0.715969\n",
      "iteration #      41, loss = 0.715692\n",
      "iteration #      42, loss = 0.715415\n",
      "iteration #      43, loss = 0.715138\n",
      "iteration #      44, loss = 0.714860\n",
      "iteration #      45, loss = 0.714583\n",
      "iteration #      46, loss = 0.714306\n",
      "iteration #      47, loss = 0.714028\n",
      "iteration #      48, loss = 0.713751\n",
      "iteration #      49, loss = 0.713473\n",
      "iteration #      50, loss = 0.713196\n",
      "iteration #      51, loss = 0.712918\n",
      "iteration #      52, loss = 0.712641\n",
      "iteration #      53, loss = 0.712363\n",
      "iteration #      54, loss = 0.712085\n",
      "iteration #      55, loss = 0.711807\n",
      "iteration #      56, loss = 0.711530\n",
      "iteration #      57, loss = 0.711252\n",
      "iteration #      58, loss = 0.710974\n",
      "iteration #      59, loss = 0.710696\n",
      "iteration #      60, loss = 0.710418\n",
      "iteration #      61, loss = 0.710140\n",
      "iteration #      62, loss = 0.709863\n",
      "iteration #      63, loss = 0.709585\n",
      "iteration #      64, loss = 0.709307\n",
      "iteration #      65, loss = 0.709029\n",
      "iteration #      66, loss = 0.708750\n",
      "iteration #      67, loss = 0.708472\n",
      "iteration #      68, loss = 0.708194\n",
      "iteration #      69, loss = 0.707916\n",
      "iteration #      70, loss = 0.707638\n",
      "iteration #      71, loss = 0.707360\n",
      "iteration #      72, loss = 0.707081\n",
      "iteration #      73, loss = 0.706803\n",
      "iteration #      74, loss = 0.706525\n",
      "iteration #      75, loss = 0.706246\n",
      "iteration #      76, loss = 0.705968\n",
      "iteration #      77, loss = 0.705690\n",
      "iteration #      78, loss = 0.705411\n",
      "iteration #      79, loss = 0.705133\n",
      "iteration #      80, loss = 0.704854\n",
      "iteration #      81, loss = 0.704576\n",
      "iteration #      82, loss = 0.704297\n",
      "iteration #      83, loss = 0.704019\n",
      "iteration #      84, loss = 0.703740\n",
      "iteration #      85, loss = 0.703462\n",
      "iteration #      86, loss = 0.703183\n",
      "iteration #      87, loss = 0.702905\n",
      "iteration #      88, loss = 0.702626\n",
      "iteration #      89, loss = 0.702347\n",
      "iteration #      90, loss = 0.702069\n",
      "iteration #      91, loss = 0.701790\n",
      "iteration #      92, loss = 0.701511\n",
      "iteration #      93, loss = 0.701232\n",
      "iteration #      94, loss = 0.700954\n",
      "iteration #      95, loss = 0.700675\n",
      "iteration #      96, loss = 0.700396\n",
      "iteration #      97, loss = 0.700117\n",
      "iteration #      98, loss = 0.699838\n",
      "iteration #      99, loss = 0.699560\n",
      "iteration #     100, loss = 0.699281\n",
      "iteration #     101, loss = 0.699002\n",
      "iteration #     102, loss = 0.698723\n",
      "iteration #     103, loss = 0.698444\n",
      "iteration #     104, loss = 0.698165\n",
      "iteration #     105, loss = 0.697886\n",
      "iteration #     106, loss = 0.697607\n",
      "iteration #     107, loss = 0.697328\n",
      "iteration #     108, loss = 0.697049\n",
      "iteration #     109, loss = 0.696770\n",
      "iteration #     110, loss = 0.696491\n",
      "iteration #     111, loss = 0.696212\n",
      "iteration #     112, loss = 0.695933\n",
      "iteration #     113, loss = 0.695654\n",
      "iteration #     114, loss = 0.695375\n",
      "iteration #     115, loss = 0.695096\n",
      "iteration #     116, loss = 0.694817\n",
      "iteration #     117, loss = 0.694538\n",
      "iteration #     118, loss = 0.694259\n",
      "iteration #     119, loss = 0.693980\n",
      "iteration #     120, loss = 0.693700\n",
      "iteration #     121, loss = 0.693421\n",
      "iteration #     122, loss = 0.693142\n",
      "iteration #     123, loss = 0.692863\n",
      "iteration #     124, loss = 0.692584\n",
      "iteration #     125, loss = 0.692305\n",
      "iteration #     126, loss = 0.692026\n",
      "iteration #     127, loss = 0.691746\n",
      "iteration #     128, loss = 0.691467\n",
      "iteration #     129, loss = 0.691188\n",
      "iteration #     130, loss = 0.690909\n",
      "iteration #     131, loss = 0.690630\n",
      "iteration #     132, loss = 0.690350\n",
      "iteration #     133, loss = 0.690071\n",
      "iteration #     134, loss = 0.689792\n",
      "iteration #     135, loss = 0.689513\n",
      "iteration #     136, loss = 0.689234\n",
      "iteration #     137, loss = 0.688955\n",
      "iteration #     138, loss = 0.688675\n",
      "iteration #     139, loss = 0.688396\n",
      "iteration #     140, loss = 0.688117\n",
      "iteration #     141, loss = 0.687838\n",
      "iteration #     142, loss = 0.687559\n",
      "iteration #     143, loss = 0.687279\n",
      "iteration #     144, loss = 0.687000\n",
      "iteration #     145, loss = 0.686721\n",
      "iteration #     146, loss = 0.686442\n",
      "iteration #     147, loss = 0.686163\n",
      "iteration #     148, loss = 0.685883\n",
      "iteration #     149, loss = 0.685604\n",
      "iteration #     150, loss = 0.685325\n",
      "iteration #     151, loss = 0.685046\n",
      "iteration #     152, loss = 0.684767\n",
      "iteration #     153, loss = 0.684487\n",
      "iteration #     154, loss = 0.684208\n",
      "iteration #     155, loss = 0.683929\n",
      "iteration #     156, loss = 0.683650\n",
      "iteration #     157, loss = 0.683371\n",
      "iteration #     158, loss = 0.683092\n",
      "iteration #     159, loss = 0.682813\n",
      "iteration #     160, loss = 0.682534\n",
      "iteration #     161, loss = 0.682254\n",
      "iteration #     162, loss = 0.681975\n",
      "iteration #     163, loss = 0.681696\n",
      "iteration #     164, loss = 0.681417\n",
      "iteration #     165, loss = 0.681138\n",
      "iteration #     166, loss = 0.680859\n",
      "iteration #     167, loss = 0.680580\n",
      "iteration #     168, loss = 0.680301\n",
      "iteration #     169, loss = 0.680022\n",
      "iteration #     170, loss = 0.679743\n",
      "iteration #     171, loss = 0.679464\n",
      "iteration #     172, loss = 0.679185\n",
      "iteration #     173, loss = 0.678906\n",
      "iteration #     174, loss = 0.678627\n",
      "iteration #     175, loss = 0.678348\n",
      "iteration #     176, loss = 0.678069\n",
      "iteration #     177, loss = 0.677790\n",
      "iteration #     178, loss = 0.677511\n",
      "iteration #     179, loss = 0.677232\n",
      "iteration #     180, loss = 0.676954\n",
      "iteration #     181, loss = 0.676675\n",
      "iteration #     182, loss = 0.676396\n",
      "iteration #     183, loss = 0.676117\n",
      "iteration #     184, loss = 0.675838\n",
      "iteration #     185, loss = 0.675560\n",
      "iteration #     186, loss = 0.675281\n",
      "iteration #     187, loss = 0.675002\n",
      "iteration #     188, loss = 0.674723\n",
      "iteration #     189, loss = 0.674445\n",
      "iteration #     190, loss = 0.674166\n",
      "iteration #     191, loss = 0.673887\n",
      "iteration #     192, loss = 0.673609\n",
      "iteration #     193, loss = 0.673330\n",
      "iteration #     194, loss = 0.673051\n",
      "iteration #     195, loss = 0.672773\n",
      "iteration #     196, loss = 0.672494\n",
      "iteration #     197, loss = 0.672216\n",
      "iteration #     198, loss = 0.671937\n",
      "iteration #     199, loss = 0.671659\n",
      "iteration #     200, loss = 0.671380\n",
      "iteration #     201, loss = 0.671102\n",
      "iteration #     202, loss = 0.670824\n",
      "iteration #     203, loss = 0.670545\n",
      "iteration #     204, loss = 0.670267\n",
      "iteration #     205, loss = 0.669989\n",
      "iteration #     206, loss = 0.669710\n",
      "iteration #     207, loss = 0.669432\n",
      "iteration #     208, loss = 0.669154\n",
      "iteration #     209, loss = 0.668876\n",
      "iteration #     210, loss = 0.668597\n",
      "iteration #     211, loss = 0.668319\n",
      "iteration #     212, loss = 0.668041\n",
      "iteration #     213, loss = 0.667763\n",
      "iteration #     214, loss = 0.667485\n",
      "iteration #     215, loss = 0.667207\n",
      "iteration #     216, loss = 0.666929\n",
      "iteration #     217, loss = 0.666651\n",
      "iteration #     218, loss = 0.666373\n",
      "iteration #     219, loss = 0.666095\n",
      "iteration #     220, loss = 0.665817\n",
      "iteration #     221, loss = 0.665540\n",
      "iteration #     222, loss = 0.665262\n",
      "iteration #     223, loss = 0.664984\n",
      "iteration #     224, loss = 0.664706\n",
      "iteration #     225, loss = 0.664429\n",
      "iteration #     226, loss = 0.664151\n",
      "iteration #     227, loss = 0.663873\n",
      "iteration #     228, loss = 0.663596\n",
      "iteration #     229, loss = 0.663318\n",
      "iteration #     230, loss = 0.663041\n",
      "iteration #     231, loss = 0.662763\n",
      "iteration #     232, loss = 0.662486\n",
      "iteration #     233, loss = 0.662208\n",
      "iteration #     234, loss = 0.661931\n",
      "iteration #     235, loss = 0.661654\n",
      "iteration #     236, loss = 0.661376\n",
      "iteration #     237, loss = 0.661099\n",
      "iteration #     238, loss = 0.660822\n",
      "iteration #     239, loss = 0.660545\n",
      "iteration #     240, loss = 0.660268\n",
      "iteration #     241, loss = 0.659991\n",
      "iteration #     242, loss = 0.659714\n",
      "iteration #     243, loss = 0.659437\n",
      "iteration #     244, loss = 0.659160\n",
      "iteration #     245, loss = 0.658883\n",
      "iteration #     246, loss = 0.658606\n",
      "iteration #     247, loss = 0.658329\n",
      "iteration #     248, loss = 0.658052\n",
      "iteration #     249, loss = 0.657776\n",
      "iteration #     250, loss = 0.657499\n",
      "iteration #     251, loss = 0.657222\n",
      "iteration #     252, loss = 0.656946\n",
      "iteration #     253, loss = 0.656669\n",
      "iteration #     254, loss = 0.656393\n",
      "iteration #     255, loss = 0.656116\n",
      "iteration #     256, loss = 0.655840\n",
      "iteration #     257, loss = 0.655563\n",
      "iteration #     258, loss = 0.655287\n",
      "iteration #     259, loss = 0.655011\n",
      "iteration #     260, loss = 0.654735\n",
      "iteration #     261, loss = 0.654458\n",
      "iteration #     262, loss = 0.654182\n",
      "iteration #     263, loss = 0.653906\n",
      "iteration #     264, loss = 0.653630\n",
      "iteration #     265, loss = 0.653354\n",
      "iteration #     266, loss = 0.653078\n",
      "iteration #     267, loss = 0.652802\n",
      "iteration #     268, loss = 0.652527\n",
      "iteration #     269, loss = 0.652251\n",
      "iteration #     270, loss = 0.651975\n",
      "iteration #     271, loss = 0.651699\n",
      "iteration #     272, loss = 0.651424\n",
      "iteration #     273, loss = 0.651148\n",
      "iteration #     274, loss = 0.650873\n",
      "iteration #     275, loss = 0.650597\n",
      "iteration #     276, loss = 0.650322\n",
      "iteration #     277, loss = 0.650047\n",
      "iteration #     278, loss = 0.649771\n",
      "iteration #     279, loss = 0.649496\n",
      "iteration #     280, loss = 0.649221\n",
      "iteration #     281, loss = 0.648946\n",
      "iteration #     282, loss = 0.648671\n",
      "iteration #     283, loss = 0.648396\n",
      "iteration #     284, loss = 0.648121\n",
      "iteration #     285, loss = 0.647846\n",
      "iteration #     286, loss = 0.647571\n",
      "iteration #     287, loss = 0.647296\n",
      "iteration #     288, loss = 0.647022\n",
      "iteration #     289, loss = 0.646747\n",
      "iteration #     290, loss = 0.646473\n",
      "iteration #     291, loss = 0.646198\n",
      "iteration #     292, loss = 0.645924\n",
      "iteration #     293, loss = 0.645649\n",
      "iteration #     294, loss = 0.645375\n",
      "iteration #     295, loss = 0.645101\n",
      "iteration #     296, loss = 0.644826\n",
      "iteration #     297, loss = 0.644552\n",
      "iteration #     298, loss = 0.644278\n",
      "iteration #     299, loss = 0.644004\n",
      "iteration #     300, loss = 0.643730\n",
      "iteration #     301, loss = 0.643456\n",
      "iteration #     302, loss = 0.643183\n",
      "iteration #     303, loss = 0.642909\n",
      "iteration #     304, loss = 0.642635\n",
      "iteration #     305, loss = 0.642361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     306, loss = 0.642088\n",
      "iteration #     307, loss = 0.641814\n",
      "iteration #     308, loss = 0.641541\n",
      "iteration #     309, loss = 0.641268\n",
      "iteration #     310, loss = 0.640994\n",
      "iteration #     311, loss = 0.640721\n",
      "iteration #     312, loss = 0.640448\n",
      "iteration #     313, loss = 0.640175\n",
      "iteration #     314, loss = 0.639902\n",
      "iteration #     315, loss = 0.639629\n",
      "iteration #     316, loss = 0.639356\n",
      "iteration #     317, loss = 0.639083\n",
      "iteration #     318, loss = 0.638811\n",
      "iteration #     319, loss = 0.638538\n",
      "iteration #     320, loss = 0.638265\n",
      "iteration #     321, loss = 0.637993\n",
      "iteration #     322, loss = 0.637720\n",
      "iteration #     323, loss = 0.637448\n",
      "iteration #     324, loss = 0.637176\n",
      "iteration #     325, loss = 0.636903\n",
      "iteration #     326, loss = 0.636631\n",
      "iteration #     327, loss = 0.636359\n",
      "iteration #     328, loss = 0.636087\n",
      "iteration #     329, loss = 0.635815\n",
      "iteration #     330, loss = 0.635543\n",
      "iteration #     331, loss = 0.635272\n",
      "iteration #     332, loss = 0.635000\n",
      "iteration #     333, loss = 0.634728\n",
      "iteration #     334, loss = 0.634457\n",
      "iteration #     335, loss = 0.634185\n",
      "iteration #     336, loss = 0.633914\n",
      "iteration #     337, loss = 0.633643\n",
      "iteration #     338, loss = 0.633371\n",
      "iteration #     339, loss = 0.633100\n",
      "iteration #     340, loss = 0.632829\n",
      "iteration #     341, loss = 0.632558\n",
      "iteration #     342, loss = 0.632287\n",
      "iteration #     343, loss = 0.632016\n",
      "iteration #     344, loss = 0.631746\n",
      "iteration #     345, loss = 0.631475\n",
      "iteration #     346, loss = 0.631204\n",
      "iteration #     347, loss = 0.630934\n",
      "iteration #     348, loss = 0.630663\n",
      "iteration #     349, loss = 0.630393\n",
      "iteration #     350, loss = 0.630123\n",
      "iteration #     351, loss = 0.629852\n",
      "iteration #     352, loss = 0.629582\n",
      "iteration #     353, loss = 0.629312\n",
      "iteration #     354, loss = 0.629042\n",
      "iteration #     355, loss = 0.628773\n",
      "iteration #     356, loss = 0.628503\n",
      "iteration #     357, loss = 0.628233\n",
      "iteration #     358, loss = 0.627964\n",
      "iteration #     359, loss = 0.627694\n",
      "iteration #     360, loss = 0.627425\n",
      "iteration #     361, loss = 0.627155\n",
      "iteration #     362, loss = 0.626886\n",
      "iteration #     363, loss = 0.626617\n",
      "iteration #     364, loss = 0.626348\n",
      "iteration #     365, loss = 0.626079\n",
      "iteration #     366, loss = 0.625810\n",
      "iteration #     367, loss = 0.625541\n",
      "iteration #     368, loss = 0.625272\n",
      "iteration #     369, loss = 0.625004\n",
      "iteration #     370, loss = 0.624735\n",
      "iteration #     371, loss = 0.624467\n",
      "iteration #     372, loss = 0.624198\n",
      "iteration #     373, loss = 0.623930\n",
      "iteration #     374, loss = 0.623662\n",
      "iteration #     375, loss = 0.623394\n",
      "iteration #     376, loss = 0.623126\n",
      "iteration #     377, loss = 0.622858\n",
      "iteration #     378, loss = 0.622590\n",
      "iteration #     379, loss = 0.622322\n",
      "iteration #     380, loss = 0.622055\n",
      "iteration #     381, loss = 0.621787\n",
      "iteration #     382, loss = 0.621520\n",
      "iteration #     383, loss = 0.621253\n",
      "iteration #     384, loss = 0.620985\n",
      "iteration #     385, loss = 0.620718\n",
      "iteration #     386, loss = 0.620451\n",
      "iteration #     387, loss = 0.620184\n",
      "iteration #     388, loss = 0.619917\n",
      "iteration #     389, loss = 0.619651\n",
      "iteration #     390, loss = 0.619384\n",
      "iteration #     391, loss = 0.619117\n",
      "iteration #     392, loss = 0.618851\n",
      "iteration #     393, loss = 0.618585\n",
      "iteration #     394, loss = 0.618318\n",
      "iteration #     395, loss = 0.618052\n",
      "iteration #     396, loss = 0.617786\n",
      "iteration #     397, loss = 0.617520\n",
      "iteration #     398, loss = 0.617254\n",
      "iteration #     399, loss = 0.616988\n",
      "iteration #     400, loss = 0.616723\n",
      "iteration #     401, loss = 0.616457\n",
      "iteration #     402, loss = 0.616192\n",
      "iteration #     403, loss = 0.615926\n",
      "iteration #     404, loss = 0.615661\n",
      "iteration #     405, loss = 0.615396\n",
      "iteration #     406, loss = 0.615131\n",
      "iteration #     407, loss = 0.614866\n",
      "iteration #     408, loss = 0.614601\n",
      "iteration #     409, loss = 0.614336\n",
      "iteration #     410, loss = 0.614072\n",
      "iteration #     411, loss = 0.613807\n",
      "iteration #     412, loss = 0.613543\n",
      "iteration #     413, loss = 0.613278\n",
      "iteration #     414, loss = 0.613014\n",
      "iteration #     415, loss = 0.612750\n",
      "iteration #     416, loss = 0.612486\n",
      "iteration #     417, loss = 0.612222\n",
      "iteration #     418, loss = 0.611958\n",
      "iteration #     419, loss = 0.611695\n",
      "iteration #     420, loss = 0.611431\n",
      "iteration #     421, loss = 0.611167\n",
      "iteration #     422, loss = 0.610904\n",
      "iteration #     423, loss = 0.610641\n",
      "iteration #     424, loss = 0.610378\n",
      "iteration #     425, loss = 0.610115\n",
      "iteration #     426, loss = 0.609852\n",
      "iteration #     427, loss = 0.609589\n",
      "iteration #     428, loss = 0.609326\n",
      "iteration #     429, loss = 0.609064\n",
      "iteration #     430, loss = 0.608801\n",
      "iteration #     431, loss = 0.608539\n",
      "iteration #     432, loss = 0.608276\n",
      "iteration #     433, loss = 0.608014\n",
      "iteration #     434, loss = 0.607752\n",
      "iteration #     435, loss = 0.607490\n",
      "iteration #     436, loss = 0.607229\n",
      "iteration #     437, loss = 0.606967\n",
      "iteration #     438, loss = 0.606705\n",
      "iteration #     439, loss = 0.606444\n",
      "iteration #     440, loss = 0.606182\n",
      "iteration #     441, loss = 0.605921\n",
      "iteration #     442, loss = 0.605660\n",
      "iteration #     443, loss = 0.605399\n",
      "iteration #     444, loss = 0.605138\n",
      "iteration #     445, loss = 0.604877\n",
      "iteration #     446, loss = 0.604617\n",
      "iteration #     447, loss = 0.604356\n",
      "iteration #     448, loss = 0.604096\n",
      "iteration #     449, loss = 0.603835\n",
      "iteration #     450, loss = 0.603575\n",
      "iteration #     451, loss = 0.603315\n",
      "iteration #     452, loss = 0.603055\n",
      "iteration #     453, loss = 0.602795\n",
      "iteration #     454, loss = 0.602536\n",
      "iteration #     455, loss = 0.602276\n",
      "iteration #     456, loss = 0.602016\n",
      "iteration #     457, loss = 0.601757\n",
      "iteration #     458, loss = 0.601498\n",
      "iteration #     459, loss = 0.601239\n",
      "iteration #     460, loss = 0.600980\n",
      "iteration #     461, loss = 0.600721\n",
      "iteration #     462, loss = 0.600462\n",
      "iteration #     463, loss = 0.600203\n",
      "iteration #     464, loss = 0.599945\n",
      "iteration #     465, loss = 0.599686\n",
      "iteration #     466, loss = 0.599428\n",
      "iteration #     467, loss = 0.599170\n",
      "iteration #     468, loss = 0.598912\n",
      "iteration #     469, loss = 0.598654\n",
      "iteration #     470, loss = 0.598396\n",
      "iteration #     471, loss = 0.598139\n",
      "iteration #     472, loss = 0.597881\n",
      "iteration #     473, loss = 0.597624\n",
      "iteration #     474, loss = 0.597366\n",
      "iteration #     475, loss = 0.597109\n",
      "iteration #     476, loss = 0.596852\n",
      "iteration #     477, loss = 0.596595\n",
      "iteration #     478, loss = 0.596339\n",
      "iteration #     479, loss = 0.596082\n",
      "iteration #     480, loss = 0.595825\n",
      "iteration #     481, loss = 0.595569\n",
      "iteration #     482, loss = 0.595313\n",
      "iteration #     483, loss = 0.595057\n",
      "iteration #     484, loss = 0.594801\n",
      "iteration #     485, loss = 0.594545\n",
      "iteration #     486, loss = 0.594289\n",
      "iteration #     487, loss = 0.594033\n",
      "iteration #     488, loss = 0.593778\n",
      "iteration #     489, loss = 0.593522\n",
      "iteration #     490, loss = 0.593267\n",
      "iteration #     491, loss = 0.593012\n",
      "iteration #     492, loss = 0.592757\n",
      "iteration #     493, loss = 0.592502\n",
      "iteration #     494, loss = 0.592247\n",
      "iteration #     495, loss = 0.591993\n",
      "iteration #     496, loss = 0.591738\n",
      "iteration #     497, loss = 0.591484\n",
      "iteration #     498, loss = 0.591230\n",
      "iteration #     499, loss = 0.590976\n",
      "iteration #     500, loss = 0.590722\n",
      "iteration #     501, loss = 0.590468\n",
      "iteration #     502, loss = 0.590214\n",
      "iteration #     503, loss = 0.589961\n",
      "iteration #     504, loss = 0.589707\n",
      "iteration #     505, loss = 0.589454\n",
      "iteration #     506, loss = 0.589201\n",
      "iteration #     507, loss = 0.588948\n",
      "iteration #     508, loss = 0.588695\n",
      "iteration #     509, loss = 0.588443\n",
      "iteration #     510, loss = 0.588190\n",
      "iteration #     511, loss = 0.587937\n",
      "iteration #     512, loss = 0.587685\n",
      "iteration #     513, loss = 0.587433\n",
      "iteration #     514, loss = 0.587181\n",
      "iteration #     515, loss = 0.586929\n",
      "iteration #     516, loss = 0.586677\n",
      "iteration #     517, loss = 0.586426\n",
      "iteration #     518, loss = 0.586174\n",
      "iteration #     519, loss = 0.585923\n",
      "iteration #     520, loss = 0.585672\n",
      "iteration #     521, loss = 0.585420\n",
      "iteration #     522, loss = 0.585170\n",
      "iteration #     523, loss = 0.584919\n",
      "iteration #     524, loss = 0.584668\n",
      "iteration #     525, loss = 0.584417\n",
      "iteration #     526, loss = 0.584167\n",
      "iteration #     527, loss = 0.583917\n",
      "iteration #     528, loss = 0.583667\n",
      "iteration #     529, loss = 0.583417\n",
      "iteration #     530, loss = 0.583167\n",
      "iteration #     531, loss = 0.582917\n",
      "iteration #     532, loss = 0.582668\n",
      "iteration #     533, loss = 0.582418\n",
      "iteration #     534, loss = 0.582169\n",
      "iteration #     535, loss = 0.581920\n",
      "iteration #     536, loss = 0.581671\n",
      "iteration #     537, loss = 0.581422\n",
      "iteration #     538, loss = 0.581173\n",
      "iteration #     539, loss = 0.580925\n",
      "iteration #     540, loss = 0.580676\n",
      "iteration #     541, loss = 0.580428\n",
      "iteration #     542, loss = 0.580180\n",
      "iteration #     543, loss = 0.579932\n",
      "iteration #     544, loss = 0.579684\n",
      "iteration #     545, loss = 0.579437\n",
      "iteration #     546, loss = 0.579189\n",
      "iteration #     547, loss = 0.578942\n",
      "iteration #     548, loss = 0.578694\n",
      "iteration #     549, loss = 0.578447\n",
      "iteration #     550, loss = 0.578200\n",
      "iteration #     551, loss = 0.577953\n",
      "iteration #     552, loss = 0.577707\n",
      "iteration #     553, loss = 0.577460\n",
      "iteration #     554, loss = 0.577214\n",
      "iteration #     555, loss = 0.576968\n",
      "iteration #     556, loss = 0.576721\n",
      "iteration #     557, loss = 0.576476\n",
      "iteration #     558, loss = 0.576230\n",
      "iteration #     559, loss = 0.575984\n",
      "iteration #     560, loss = 0.575739\n",
      "iteration #     561, loss = 0.575493\n",
      "iteration #     562, loss = 0.575248\n",
      "iteration #     563, loss = 0.575003\n",
      "iteration #     564, loss = 0.574758\n",
      "iteration #     565, loss = 0.574513\n",
      "iteration #     566, loss = 0.574269\n",
      "iteration #     567, loss = 0.574024\n",
      "iteration #     568, loss = 0.573780\n",
      "iteration #     569, loss = 0.573536\n",
      "iteration #     570, loss = 0.573292\n",
      "iteration #     571, loss = 0.573048\n",
      "iteration #     572, loss = 0.572804\n",
      "iteration #     573, loss = 0.572560\n",
      "iteration #     574, loss = 0.572317\n",
      "iteration #     575, loss = 0.572074\n",
      "iteration #     576, loss = 0.571831\n",
      "iteration #     577, loss = 0.571588\n",
      "iteration #     578, loss = 0.571345\n",
      "iteration #     579, loss = 0.571102\n",
      "iteration #     580, loss = 0.570860\n",
      "iteration #     581, loss = 0.570617\n",
      "iteration #     582, loss = 0.570375\n",
      "iteration #     583, loss = 0.570133\n",
      "iteration #     584, loss = 0.569891\n",
      "iteration #     585, loss = 0.569649\n",
      "iteration #     586, loss = 0.569408\n",
      "iteration #     587, loss = 0.569166\n",
      "iteration #     588, loss = 0.568925\n",
      "iteration #     589, loss = 0.568684\n",
      "iteration #     590, loss = 0.568443\n",
      "iteration #     591, loss = 0.568202\n",
      "iteration #     592, loss = 0.567961\n",
      "iteration #     593, loss = 0.567721\n",
      "iteration #     594, loss = 0.567480\n",
      "iteration #     595, loss = 0.567240\n",
      "iteration #     596, loss = 0.567000\n",
      "iteration #     597, loss = 0.566760\n",
      "iteration #     598, loss = 0.566520\n",
      "iteration #     599, loss = 0.566281\n",
      "iteration #     600, loss = 0.566041\n",
      "iteration #     601, loss = 0.565802\n",
      "iteration #     602, loss = 0.565563\n",
      "iteration #     603, loss = 0.565324\n",
      "iteration #     604, loss = 0.565085\n",
      "iteration #     605, loss = 0.564846\n",
      "iteration #     606, loss = 0.564608\n",
      "iteration #     607, loss = 0.564369\n",
      "iteration #     608, loss = 0.564131\n",
      "iteration #     609, loss = 0.563893\n",
      "iteration #     610, loss = 0.563655\n",
      "iteration #     611, loss = 0.563418\n",
      "iteration #     612, loss = 0.563180\n",
      "iteration #     613, loss = 0.562942\n",
      "iteration #     614, loss = 0.562705\n",
      "iteration #     615, loss = 0.562468\n",
      "iteration #     616, loss = 0.562231\n",
      "iteration #     617, loss = 0.561994\n",
      "iteration #     618, loss = 0.561758\n",
      "iteration #     619, loss = 0.561521\n",
      "iteration #     620, loss = 0.561285\n",
      "iteration #     621, loss = 0.561049\n",
      "iteration #     622, loss = 0.560813\n",
      "iteration #     623, loss = 0.560577\n",
      "iteration #     624, loss = 0.560341\n",
      "iteration #     625, loss = 0.560106\n",
      "iteration #     626, loss = 0.559870\n",
      "iteration #     627, loss = 0.559635\n",
      "iteration #     628, loss = 0.559400\n",
      "iteration #     629, loss = 0.559165\n",
      "iteration #     630, loss = 0.558930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #     631, loss = 0.558696\n",
      "iteration #     632, loss = 0.558461\n",
      "iteration #     633, loss = 0.558227\n",
      "iteration #     634, loss = 0.557993\n",
      "iteration #     635, loss = 0.557759\n",
      "iteration #     636, loss = 0.557525\n",
      "iteration #     637, loss = 0.557292\n",
      "iteration #     638, loss = 0.557058\n",
      "iteration #     639, loss = 0.556825\n",
      "iteration #     640, loss = 0.556592\n",
      "iteration #     641, loss = 0.556359\n",
      "iteration #     642, loss = 0.556126\n",
      "iteration #     643, loss = 0.555893\n",
      "iteration #     644, loss = 0.555661\n",
      "iteration #     645, loss = 0.555428\n",
      "iteration #     646, loss = 0.555196\n",
      "iteration #     647, loss = 0.554964\n",
      "iteration #     648, loss = 0.554732\n",
      "iteration #     649, loss = 0.554501\n",
      "iteration #     650, loss = 0.554269\n",
      "iteration #     651, loss = 0.554038\n",
      "iteration #     652, loss = 0.553807\n",
      "iteration #     653, loss = 0.553576\n",
      "iteration #     654, loss = 0.553345\n",
      "iteration #     655, loss = 0.553114\n",
      "iteration #     656, loss = 0.552883\n",
      "iteration #     657, loss = 0.552653\n",
      "iteration #     658, loss = 0.552423\n",
      "iteration #     659, loss = 0.552193\n",
      "iteration #     660, loss = 0.551963\n",
      "iteration #     661, loss = 0.551733\n",
      "iteration #     662, loss = 0.551504\n",
      "iteration #     663, loss = 0.551274\n",
      "iteration #     664, loss = 0.551045\n",
      "iteration #     665, loss = 0.550816\n",
      "iteration #     666, loss = 0.550587\n",
      "iteration #     667, loss = 0.550358\n",
      "iteration #     668, loss = 0.550130\n",
      "iteration #     669, loss = 0.549901\n",
      "iteration #     670, loss = 0.549673\n",
      "iteration #     671, loss = 0.549445\n",
      "iteration #     672, loss = 0.549217\n",
      "iteration #     673, loss = 0.548989\n",
      "iteration #     674, loss = 0.548761\n",
      "iteration #     675, loss = 0.548534\n",
      "iteration #     676, loss = 0.548307\n",
      "iteration #     677, loss = 0.548080\n",
      "iteration #     678, loss = 0.547853\n",
      "iteration #     679, loss = 0.547626\n",
      "iteration #     680, loss = 0.547399\n",
      "iteration #     681, loss = 0.547173\n",
      "iteration #     682, loss = 0.546946\n",
      "iteration #     683, loss = 0.546720\n",
      "iteration #     684, loss = 0.546494\n",
      "iteration #     685, loss = 0.546269\n",
      "iteration #     686, loss = 0.546043\n",
      "iteration #     687, loss = 0.545817\n",
      "iteration #     688, loss = 0.545592\n",
      "iteration #     689, loss = 0.545367\n",
      "iteration #     690, loss = 0.545142\n",
      "iteration #     691, loss = 0.544917\n",
      "iteration #     692, loss = 0.544693\n",
      "iteration #     693, loss = 0.544468\n",
      "iteration #     694, loss = 0.544244\n",
      "iteration #     695, loss = 0.544020\n",
      "iteration #     696, loss = 0.543796\n",
      "iteration #     697, loss = 0.543572\n",
      "iteration #     698, loss = 0.543348\n",
      "iteration #     699, loss = 0.543125\n",
      "iteration #     700, loss = 0.542901\n",
      "iteration #     701, loss = 0.542678\n",
      "iteration #     702, loss = 0.542455\n",
      "iteration #     703, loss = 0.542233\n",
      "iteration #     704, loss = 0.542010\n",
      "iteration #     705, loss = 0.541787\n",
      "iteration #     706, loss = 0.541565\n",
      "iteration #     707, loss = 0.541343\n",
      "iteration #     708, loss = 0.541121\n",
      "iteration #     709, loss = 0.540899\n",
      "iteration #     710, loss = 0.540678\n",
      "iteration #     711, loss = 0.540456\n",
      "iteration #     712, loss = 0.540235\n",
      "iteration #     713, loss = 0.540014\n",
      "iteration #     714, loss = 0.539793\n",
      "iteration #     715, loss = 0.539572\n",
      "iteration #     716, loss = 0.539351\n",
      "iteration #     717, loss = 0.539131\n",
      "iteration #     718, loss = 0.538910\n",
      "iteration #     719, loss = 0.538690\n",
      "iteration #     720, loss = 0.538470\n",
      "iteration #     721, loss = 0.538250\n",
      "iteration #     722, loss = 0.538031\n",
      "iteration #     723, loss = 0.537811\n",
      "iteration #     724, loss = 0.537592\n",
      "iteration #     725, loss = 0.537373\n",
      "iteration #     726, loss = 0.537154\n",
      "iteration #     727, loss = 0.536935\n",
      "iteration #     728, loss = 0.536717\n",
      "iteration #     729, loss = 0.536498\n",
      "iteration #     730, loss = 0.536280\n",
      "iteration #     731, loss = 0.536062\n",
      "iteration #     732, loss = 0.535844\n",
      "iteration #     733, loss = 0.535626\n",
      "iteration #     734, loss = 0.535408\n",
      "iteration #     735, loss = 0.535191\n",
      "iteration #     736, loss = 0.534974\n",
      "iteration #     737, loss = 0.534757\n",
      "iteration #     738, loss = 0.534540\n",
      "iteration #     739, loss = 0.534323\n",
      "iteration #     740, loss = 0.534106\n",
      "iteration #     741, loss = 0.533890\n",
      "iteration #     742, loss = 0.533673\n",
      "iteration #     743, loss = 0.533457\n",
      "iteration #     744, loss = 0.533241\n",
      "iteration #     745, loss = 0.533026\n",
      "iteration #     746, loss = 0.532810\n",
      "iteration #     747, loss = 0.532595\n",
      "iteration #     748, loss = 0.532379\n",
      "iteration #     749, loss = 0.532164\n",
      "iteration #     750, loss = 0.531949\n",
      "iteration #     751, loss = 0.531735\n",
      "iteration #     752, loss = 0.531520\n",
      "iteration #     753, loss = 0.531306\n",
      "iteration #     754, loss = 0.531091\n",
      "iteration #     755, loss = 0.530877\n",
      "iteration #     756, loss = 0.530663\n",
      "iteration #     757, loss = 0.530450\n",
      "iteration #     758, loss = 0.530236\n",
      "iteration #     759, loss = 0.530023\n",
      "iteration #     760, loss = 0.529810\n",
      "iteration #     761, loss = 0.529596\n",
      "iteration #     762, loss = 0.529384\n",
      "iteration #     763, loss = 0.529171\n",
      "iteration #     764, loss = 0.528958\n",
      "iteration #     765, loss = 0.528746\n",
      "iteration #     766, loss = 0.528534\n",
      "iteration #     767, loss = 0.528322\n",
      "iteration #     768, loss = 0.528110\n",
      "iteration #     769, loss = 0.527898\n",
      "iteration #     770, loss = 0.527687\n",
      "iteration #     771, loss = 0.527475\n",
      "iteration #     772, loss = 0.527264\n",
      "iteration #     773, loss = 0.527053\n",
      "iteration #     774, loss = 0.526842\n",
      "iteration #     775, loss = 0.526631\n",
      "iteration #     776, loss = 0.526421\n",
      "iteration #     777, loss = 0.526211\n",
      "iteration #     778, loss = 0.526000\n",
      "iteration #     779, loss = 0.525790\n",
      "iteration #     780, loss = 0.525581\n",
      "iteration #     781, loss = 0.525371\n",
      "iteration #     782, loss = 0.525161\n",
      "iteration #     783, loss = 0.524952\n",
      "iteration #     784, loss = 0.524743\n",
      "iteration #     785, loss = 0.524534\n",
      "iteration #     786, loss = 0.524325\n",
      "iteration #     787, loss = 0.524116\n",
      "iteration #     788, loss = 0.523908\n",
      "iteration #     789, loss = 0.523700\n",
      "iteration #     790, loss = 0.523491\n",
      "iteration #     791, loss = 0.523284\n",
      "iteration #     792, loss = 0.523076\n",
      "iteration #     793, loss = 0.522868\n",
      "iteration #     794, loss = 0.522661\n",
      "iteration #     795, loss = 0.522453\n",
      "iteration #     796, loss = 0.522246\n",
      "iteration #     797, loss = 0.522039\n",
      "iteration #     798, loss = 0.521832\n",
      "iteration #     799, loss = 0.521626\n",
      "iteration #     800, loss = 0.521419\n",
      "iteration #     801, loss = 0.521213\n",
      "iteration #     802, loss = 0.521007\n",
      "iteration #     803, loss = 0.520801\n",
      "iteration #     804, loss = 0.520595\n",
      "iteration #     805, loss = 0.520390\n",
      "iteration #     806, loss = 0.520184\n",
      "iteration #     807, loss = 0.519979\n",
      "iteration #     808, loss = 0.519774\n",
      "iteration #     809, loss = 0.519569\n",
      "iteration #     810, loss = 0.519364\n",
      "iteration #     811, loss = 0.519160\n",
      "iteration #     812, loss = 0.518955\n",
      "iteration #     813, loss = 0.518751\n",
      "iteration #     814, loss = 0.518547\n",
      "iteration #     815, loss = 0.518343\n",
      "iteration #     816, loss = 0.518139\n",
      "iteration #     817, loss = 0.517936\n",
      "iteration #     818, loss = 0.517732\n",
      "iteration #     819, loss = 0.517529\n",
      "iteration #     820, loss = 0.517326\n",
      "iteration #     821, loss = 0.517123\n",
      "iteration #     822, loss = 0.516921\n",
      "iteration #     823, loss = 0.516718\n",
      "iteration #     824, loss = 0.516516\n",
      "iteration #     825, loss = 0.516314\n",
      "iteration #     826, loss = 0.516112\n",
      "iteration #     827, loss = 0.515910\n",
      "iteration #     828, loss = 0.515708\n",
      "iteration #     829, loss = 0.515506\n",
      "iteration #     830, loss = 0.515305\n",
      "iteration #     831, loss = 0.515104\n",
      "iteration #     832, loss = 0.514903\n",
      "iteration #     833, loss = 0.514702\n",
      "iteration #     834, loss = 0.514501\n",
      "iteration #     835, loss = 0.514301\n",
      "iteration #     836, loss = 0.514101\n",
      "iteration #     837, loss = 0.513900\n",
      "iteration #     838, loss = 0.513700\n",
      "iteration #     839, loss = 0.513501\n",
      "iteration #     840, loss = 0.513301\n",
      "iteration #     841, loss = 0.513102\n",
      "iteration #     842, loss = 0.512902\n",
      "iteration #     843, loss = 0.512703\n",
      "iteration #     844, loss = 0.512504\n",
      "iteration #     845, loss = 0.512305\n",
      "iteration #     846, loss = 0.512107\n",
      "iteration #     847, loss = 0.511908\n",
      "iteration #     848, loss = 0.511710\n",
      "iteration #     849, loss = 0.511512\n",
      "iteration #     850, loss = 0.511314\n",
      "iteration #     851, loss = 0.511116\n",
      "iteration #     852, loss = 0.510918\n",
      "iteration #     853, loss = 0.510721\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(1, x_training_aug.shape[1])\n",
    "theta = bgd(x_training_aug, y_training, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the parameters are learned, we can test the performance of the classifier. Note that logistic regression returns a number in [0,1], thus we need to binarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression test error = 0.6417\n"
     ]
    }
   ],
   "source": [
    "y_prediction = (h(x_testing_aug, theta)>=0.5)[:,0]\n",
    "test_error = np.sum(y_testing != y_prediction) / y_testing.shape[0]\n",
    "print('logistic regression test error = {:.4f}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-930beaa92507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_index_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "print(feature_index_names[0].value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
